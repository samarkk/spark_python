{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['SPARK_HOME']='/usr/lib/spark'\n",
    "os.environ['PYLIB']=os.environ['SPARK_HOME']+'/python/lib'\n",
    "sys.path.insert(0,os.environ['PYLIB']+'/py4j-0.10.7-src.zip')\n",
    "sys.path.insert(1,os.environ['PYLIB']+'/pyspark.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('TestHive') \\\n",
    ".config('spark.warehouse.dir','/apps/hive/warehouse') \\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Having set the driver and driver options we should have spark representing spark session \n",
    "# available straight away\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+\n",
      "|database|           tableName|isTemporary|\n",
      "+--------+--------------------+-----------+\n",
      "| default|          categories|      false|\n",
      "| default|            cmfvwtbl|      false|\n",
      "| default|           customers|      false|\n",
      "| default|            demtable|      false|\n",
      "| default|         departments|      false|\n",
      "| default|                 hkt|      false|\n",
      "| default|intermediate_acce...|      false|\n",
      "| default|      my_first_table|      false|\n",
      "| default|               nsecm|      false|\n",
      "| default|    ntest_kudu_table|      false|\n",
      "| default|          oitem_kudu|      false|\n",
      "| default|         order_items|      false|\n",
      "| default|              orders|      false|\n",
      "| default|        product_kudu|      false|\n",
      "| default|            products|      false|\n",
      "| default|          spark2ttbl|      false|\n",
      "| default|spark_kudu_mappin...|      false|\n",
      "| default|                spkt|      false|\n",
      "| default|    spkt_mapping_tbl|      false|\n",
      "| default|             stmdtbl|      false|\n",
      "+--------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 1), ('c', 1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_coll = ['a', 'b', 'c']\n",
    "a_coll_rdd = sc.parallelize(a_coll)\n",
    "a_coll_rdd.map(lambda x: (x, 1)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering to print only odd numbers \n",
      " [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79, 81, 83, 85, 87, 89, 91, 93, 95, 97, 99]\n"
     ]
    }
   ],
   "source": [
    "no_rdd = sc.parallelize(range(100))\n",
    "print('Filtering to print only odd numbers \\n', no_rdd.filter(lambda x: x % 2 != 0).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flatmapping illustration \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 100, 42, 2, 200, 42, 3, 300, 42]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ott_rdd = sc.parallelize([1, 2, 3])\n",
    "print('Flatmapping illustration \\n')\n",
    "ott_rdd.flatMap(lambda x: (x, x * 100, 42)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD groupBy transformation \n"
     ]
    }
   ],
   "source": [
    "print('RDD groupBy transformation ')\n",
    "group_by_collect = no_rdd.groupBy(lambda x: x % 2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [0,\n",
       "   2,\n",
       "   4,\n",
       "   6,\n",
       "   8,\n",
       "   10,\n",
       "   12,\n",
       "   14,\n",
       "   16,\n",
       "   18,\n",
       "   20,\n",
       "   22,\n",
       "   24,\n",
       "   26,\n",
       "   28,\n",
       "   30,\n",
       "   32,\n",
       "   34,\n",
       "   36,\n",
       "   38,\n",
       "   40,\n",
       "   42,\n",
       "   44,\n",
       "   46,\n",
       "   48,\n",
       "   50,\n",
       "   52,\n",
       "   54,\n",
       "   56,\n",
       "   58,\n",
       "   60,\n",
       "   62,\n",
       "   64,\n",
       "   66,\n",
       "   68,\n",
       "   70,\n",
       "   72,\n",
       "   74,\n",
       "   76,\n",
       "   78,\n",
       "   80,\n",
       "   82,\n",
       "   84,\n",
       "   86,\n",
       "   88,\n",
       "   90,\n",
       "   92,\n",
       "   94,\n",
       "   96,\n",
       "   98]),\n",
       " (1,\n",
       "  [1,\n",
       "   3,\n",
       "   5,\n",
       "   7,\n",
       "   9,\n",
       "   11,\n",
       "   13,\n",
       "   15,\n",
       "   17,\n",
       "   19,\n",
       "   21,\n",
       "   23,\n",
       "   25,\n",
       "   27,\n",
       "   29,\n",
       "   31,\n",
       "   33,\n",
       "   35,\n",
       "   37,\n",
       "   39,\n",
       "   41,\n",
       "   43,\n",
       "   45,\n",
       "   47,\n",
       "   49,\n",
       "   51,\n",
       "   53,\n",
       "   55,\n",
       "   57,\n",
       "   59,\n",
       "   61,\n",
       "   63,\n",
       "   65,\n",
       "   67,\n",
       "   69,\n",
       "   71,\n",
       "   73,\n",
       "   75,\n",
       "   77,\n",
       "   79,\n",
       "   81,\n",
       "   83,\n",
       "   85,\n",
       "   87,\n",
       "   89,\n",
       "   91,\n",
       "   93,\n",
       "   95,\n",
       "   97,\n",
       "   99])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(x[0], list(x[1])) for x in group_by_collect]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utillity function to print tuples in a list\n",
    "def printTupleList(tlist):\n",
    "    return [(x[0], list(x[1])) for x in tlist ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a rdd to illustrate groupByKey transformation\n",
    "pow_lt3_rdd = sc.parallelize([(1, 1), (1, 1), (1, 1), (2, 2), (2, 4), (2, 8), (3, 3), (3, 9), (3, 27)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "groupByKey transformation\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(2, [2, 4, 8]), (1, [1, 1, 1]), (3, [3, 9, 27])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('groupByKey transformation\\n')\n",
    "printTupleList(pow_lt3_rdd.groupByKey().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduceByKey transformation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(2, 14), (1, 3), (3, 39)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('reduceByKey transformation')\n",
    "pow_lt3_rdd.reduceByKey(lambda x, y: x + y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_coll_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a'], ['b', 'c']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_coll_rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_partitions(idx, itera):\n",
    "    yield 'index: ' + str(idx) + ' , elements: ' + str(list(itera))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapPartitionsWithIndex \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"index: 0 , elements: ['a']\", \"index: 1 , elements: ['b', 'c']\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('mapPartitionsWithIndex ')\n",
    "a_coll_rdd.mapPartitionsWithIndex(show_partitions).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(sidx, itr): yield sidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_coll_rdd.mapPartitionsWithIndex(f).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we require a function to iterate over the elements of a partition\n",
    "def map_partitions_function(itera): \n",
    "    yield ', '.join(list(itera))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b, c']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_coll_rdd.mapPartitions(map_partitions_function).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pair RDD Operations - Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Union of two rdds:\n",
      "\n",
      "union:  [1, 2, 3, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "print('Union of two rdds:\\n')\n",
    "xu = sc.parallelize([1, 2, 3], 2)\n",
    "yu = sc.parallelize([3, 4], 1)\n",
    "print(\"union: \" , xu.union(yu).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', (2, 5)), ('a', (1, 3)), ('a', (1, 4))]\n"
     ]
    }
   ],
   "source": [
    "xj = sc.parallelize([(\"a\", 1), (\"b\", 2)])\n",
    "yj = sc.parallelize([(\"a\", 3), (\"a\", 4), (\"b\", 5)])\n",
    "print(xj.join(yj).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left Outer Join\n",
      "[('b', (2, 5)), ('c', (7, None)), ('a', (1, 3)), ('a', (1, 4))]\n",
      "Right Outer Join\n",
      "[('b', (2, 5)), ('a', (1, 3)), ('a', (1, 4)), ('d', (None, 4))]\n"
     ]
    }
   ],
   "source": [
    "print(\"Left Outer Join\")\n",
    "xoj = sc.parallelize([(\"a\", 1), (\"b\", 2), (\"c\", 7)])\n",
    "yoj = sc.parallelize([(\"a\", 3), (\"a\", 4), (\"b\", 5), (\"d\", 4)])\n",
    "print(xoj.leftOuterJoin(yoj).collect())\n",
    "print(\"Right Outer Join\")\n",
    "print(xoj.rightOuterJoin(yoj).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Outer Join\n",
      "[('b', (2, 5)), ('c', (7, None)), ('a', (1, 3)), ('a', (1, 4)), ('d', (None, 4))]\n"
     ]
    }
   ],
   "source": [
    "print(\"Full Outer Join\")\n",
    "print(xoj.fullOuterJoin(yoj).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct\n",
      "\n",
      "Randomly generated list with repeat elements  [1 0 0 3 0 0 1 1 4 0]\n",
      "[0, 4, 1, 3]\n"
     ]
    }
   ],
   "source": [
    "print('Distinct\\n')\n",
    "rpt_list = np.random.randint(0, 5, 10)\n",
    "print('Randomly generated list with repeat elements ', rpt_list)\n",
    "rpt_rdd = sc.parallelize(rpt_list)\n",
    "print(rpt_rdd.distinct().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coalesce to reduce number of partitions\n",
      "4\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(\"coalesce to reduce number of partitions\")\n",
    "xCoalesce = sc.parallelize(range(10), 4)\n",
    "print(xCoalesce.getNumPartitions())\n",
    "yCoalesce = xCoalesce.coalesce(2)\n",
    "print(yCoalesce.getNumPartitions())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyBy transformation\n",
      "[(True, 0), (False, 1), (True, 2), (False, 3), (True, 4), (False, 5), (True, 6), (False, 7), (True, 8), (False, 9)]\n",
      "[('even', 0), ('odd', 1), ('even', 2), ('odd', 3), ('even', 4), ('odd', 5), ('even', 6), ('odd', 7), ('even', 8), ('odd', 9)]\n"
     ]
    }
   ],
   "source": [
    "print('keyBy transformation')\n",
    "new_no_rdd = sc.parallelize(range(10))\n",
    "print(new_no_rdd.keyBy(lambda x: x % 2 == 0).collect())\n",
    "print(new_no_rdd.keyBy(lambda x: x % 2 == 0).map(lambda x: ('even' if x[0] else 'odd', x[1])).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Partitioner Spark RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_function(k):\n",
    "    return 0 if k < 'H' else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"index: 0 , elements: [('F', 'Fred'), ('A', 'Anna')]\",\n",
       " \"index: 1 , elements: [('J', 'James'), ('J', 'John')]\"]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_part = sc.parallelize([('J', \"James\"), ('F', \"Fred\"), ('A', \"Anna\"), ('J', \"John\")], 3)\n",
    "x_part.partitionBy(2, part_function).mapPartitionsWithIndex(show_partitions).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping RDDs\n",
      "[(0, 1), (1, 4), (2, 9), (3, 16)]\n"
     ]
    }
   ],
   "source": [
    "azip = sc.parallelize(range(4))\n",
    "bzip = sc.parallelize([1, 4, 9, 16])\n",
    "\n",
    "print(\"Zipping RDDs\")\n",
    "print(azip.zip(bzip).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 17, 14, 2, 15, 13, 12, 4, 1, 19]\n"
     ]
    }
   ],
   "source": [
    "rand_rdd = sc.parallelize(np.random.randint(0, 20, 10))\n",
    ";print(rand_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple sort ascending\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 4, 9, 12, 13, 14, 15, 17, 19]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Simple sort ascending')\n",
    "rand_rdd.sortBy(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple sort descending\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[19, 17, 15, 14, 13, 12, 9, 4, 2, 1]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Simple sort descending')\n",
    "rand_rdd.sortBy(lambda x: -x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_rdd = sc.parallelize([(\"arjun\", \"tendulkar\", 5),\n",
    "    (\"sachin\", \"tendulkar\", 102), (\"vachin\", \"tendulkar\", 102),\n",
    "    (\"rahul\", \"dravid\", 74), (\"vahul\", \"dravid\", 74),\n",
    "    (\"rahul\", \"shavid\", 74), (\"vahul\", \"shavid\", 74),\n",
    "    (\"jacques\", \"kallis\", 92), (\"ricky\", \"ponting\", 84), (\"jacques\", \"zaalim\", 92),\n",
    "    (\"sachin\", \"vendulkar\", 102)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('jacques', 'zaalim', 92),\n",
       " ('sachin', 'vendulkar', 102),\n",
       " ('arjun', 'tendulkar', 5),\n",
       " ('sachin', 'tendulkar', 102),\n",
       " ('vachin', 'tendulkar', 102),\n",
       " ('rahul', 'shavid', 74),\n",
       " ('vahul', 'shavid', 74),\n",
       " ('ricky', 'ponting', 84),\n",
       " ('jacques', 'kallis', 92),\n",
       " ('rahul', 'dravid', 74),\n",
       " ('vahul', 'dravid', 74)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_rdd.sortBy(lambda x: x[1], False).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+\n",
      "|  fname|    lname|centuries|\n",
      "+-------+---------+---------+\n",
      "|  arjun|tendulkar|        5|\n",
      "| sachin|tendulkar|      102|\n",
      "| vachin|tendulkar|      102|\n",
      "|  rahul|   dravid|       74|\n",
      "|  vahul|   dravid|       74|\n",
      "|  rahul|   shavid|       74|\n",
      "|  vahul|   shavid|       74|\n",
      "|jacques|   kallis|       92|\n",
      "|  ricky|  ponting|       84|\n",
      "|jacques|   zaalim|       92|\n",
      "| sachin|vendulkar|      102|\n",
      "+-------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comp_df = comp_rdd.toDF(['fname', 'lname', 'centuries'])\n",
    "comp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+\n",
      "|  fname|    lname|centuries|\n",
      "+-------+---------+---------+\n",
      "|  vahul|   dravid|       74|\n",
      "|  rahul|   dravid|       74|\n",
      "|jacques|   kallis|       92|\n",
      "|  ricky|  ponting|       84|\n",
      "|  vahul|   shavid|       74|\n",
      "|  rahul|   shavid|       74|\n",
      "| vachin|tendulkar|      102|\n",
      "| sachin|tendulkar|      102|\n",
      "|  arjun|tendulkar|        5|\n",
      "| sachin|vendulkar|      102|\n",
      "|jacques|   zaalim|       92|\n",
      "+-------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "comp_df.sort('lname', desc('fname'), desc('centuries')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD Aggregation\n",
    "### A sequence operation which will run as a combiner of sort on the elements of the partition\n",
    "### A combining operation which will reduce the combined tuples from the partitions to a single tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First take a look at the partitions of the rdd\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'show_partitions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b1c93e6653f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mardd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'First take a look at the partitions of the rdd'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mardd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitionsWithIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow_partitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mardd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomb_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'show_partitions' is not defined"
     ]
    }
   ],
   "source": [
    "def seq_op(data, elem):\n",
    "    return (data[0] + elem, data[1] + 1)\n",
    "\n",
    "def comb_op(d1, d2):\n",
    "    return (d1[0] + d2[0], d1[1] + d2[1])\n",
    "\n",
    "ardd = sc.parallelize(range(1, 13), 6)\n",
    "print('First take a look at the partitions of the rdd')\n",
    "print(ardd.mapPartitionsWithIndex(show_partitions).collect())\n",
    "print(ardd.aggregate((0,0), seq_op, comb_op))\n",
    "\n",
    "print(ardd.aggregate((0, 0), \n",
    "                    lambda data, elem: (data[0] + elem, data[1] + 1),\n",
    "                    lambda d1, d2: (d1[0] + d2[0], d1[1] + d2[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Tree aggregations operate exactly in the same way as aggreate  except for one critical difference - there is an intermediate aggregation step  data from some partitions will be sent to executors to aggregate\n",
    " ### so in the above case if there are six partitions  while aggregate will send results of all the six partitions to the driver in tree aggregate, three will go to one executor, three to another and the driver will receive the aggregations from 2 rather than 6\n",
    " ### Where there are many number of partitions tree aggregate performs significantly better than vanilla aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78, 12)\n"
     ]
    }
   ],
   "source": [
    "print(ardd.treeAggregate((0,0), seq_op, comb_op))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "frd = sc.parallelize([(1, 1), (2, 4), (3, 9), (1, 1), (2, 8), (3, 27),\n",
    "      (1, 1), (2, 16), (3, 81), (4, 256)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, (28, 3)), (4, (256, 1)), (1, (3, 3)), (3, (117, 3))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frd.combineByKey(lambda x: (x, 1),\n",
    "                lambda acc, vlu: (acc[0] + vlu, acc[1] + 1),  \n",
    "                lambda v1, v2: (v1[0] + v2[0], v1[1] + v2[1])).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aggregateByKey also will function at the partition level - we need a value to seed the aggregation and we have a combination of a sequence operation and a combo operation playing out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 28), (4, 256), (1, 3), (3, 117)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frd.aggregateByKey(0,\n",
    "                   lambda x, y: x + y, \n",
    "                   lambda a, b: a + b\n",
    "                  ).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

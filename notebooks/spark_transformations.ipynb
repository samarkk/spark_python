{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['SPARK_HOME']='D:/spark330hdp3sc3'\n",
    "os.environ['PYLIB']=os.environ['SPARK_HOME']+'/python/lib'\n",
    "sys.path.insert(0,os.environ['PYLIB']+'/py4j-0.10.9.5-src.zip')\n",
    "sys.path.insert(1,os.environ['PYLIB']+'/pyspark.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('SparkTransformations') \\\n",
    ".config('spark.warehouse.dir','D:/tmp') \\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Having set the driver and driver options we should have spark representing spark session \n",
    "# available straight away\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 1), ('c', 1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD.map(f: Callable[[T], U], preservesPartitioning: bool = False) → pyspark.rdd.RDD[U]\n",
    "a_coll = ['a', 'b', 'c']\n",
    "a_coll_rdd = sc.parallelize(a_coll)\n",
    "a_coll_rdd.map(lambda x: (x, 1)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering to print only odd numbers \n",
      " [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79, 81, 83, 85, 87, 89, 91, 93, 95, 97, 99]\n"
     ]
    }
   ],
   "source": [
    "# RDD.filter(f: Callable[[T], bool]) → pyspark.rdd.RDD[T][source]\n",
    "no_rdd = sc.parallelize(range(100))\n",
    "print('Filtering to print only odd numbers \\n', no_rdd.filter(lambda x: x % 2 != 0).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flatmapping illustration \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 100, 42, 2, 200, 42, 3, 300, 42]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD.flatMap(f: Callable[[T], Iterable[U]], preservesPartitioning: bool = False) → pyspark.rdd.RDD[U]\n",
    "ott_rdd = sc.parallelize([1, 2, 3])\n",
    "print('Flatmapping illustration \\n')\n",
    "ott_rdd.flatMap(lambda x: (x, x * 100, 42)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD groupBy transformation \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "RDD.groupBy(f: Callable[[T], K], numPartitions: Optional[int] = None, \n",
    "partitionFunc: Callable[[K], int] = <function portable_hash>) → pyspark.rdd.RDD[Tuple[K, Iterable[T]]]\n",
    "'''\n",
    "print('RDD groupBy transformation ')\n",
    "group_by_collect = no_rdd.groupBy(lambda x: x % 2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98]), (1, [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79, 81, 83, 85, 87, 89, 91, 93, 95, 97, 99])]\n"
     ]
    }
   ],
   "source": [
    "print([(x[0], list(x[1])) for x in group_by_collect])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utillity function to print tuples in a list\n",
    "def printTupleList(tlist):\n",
    "    return [(x[0], list(x[1])) for x in tlist ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a rdd to illustrate groupByKey transformation\n",
    "pow_lt3_rdd = sc.parallelize([(1, 1), (1, 1), (1, 1), (2, 2), (2, 4), (2, 8), (3, 3), (3, 9), (3, 27)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "groupByKey transformation\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, [1, 1, 1]), (2, [2, 4, 8]), (3, [3, 9, 27])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "RDD.groupByKey(numPartitions: Optional[int] = None, \n",
    "partitionFunc: Callable[[K], int] = <function portable_hash>) → pyspark.rdd.RDD[Tuple[K, Iterable[V]]]\n",
    "'''\n",
    "print('groupByKey transformation\\n')\n",
    "printTupleList(pow_lt3_rdd.groupByKey().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduceByKey transformation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 3), (2, 14), (3, 39)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "RDD.reduceByKey(func: Callable[[V, V], V], numPartitions: Optional[int] = None, \n",
    "partitionFunc: Callable[[K], int] = <function portable_hash>) →  pyspark.rdd.RDD[Tuple[K, V]]\n",
    "'''\n",
    "print('reduceByKey transformation')\n",
    "pow_lt3_rdd.reduceByKey(lambda x, y: x + y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_coll_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [], ['a'], [], [], ['b'], [], ['c']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD.glom() → pyspark.rdd.RDD[List[T]]\n",
    "a_coll_rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD.mapPartitions(f: Callable[[Iterable[T]], Iterable[U]], preservesPartitioning: bool = False) → pyspark.rdd.RDD[U]\n",
    "def show_partitions(idx, itera):\n",
    "    yield 'index: ' + str(idx) + ' , elements: ' + str(list(itera))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapPartitionsWithIndex \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['index: 0 , elements: []',\n",
       " 'index: 1 , elements: []',\n",
       " \"index: 2 , elements: ['a']\",\n",
       " 'index: 3 , elements: []',\n",
       " 'index: 4 , elements: []',\n",
       " \"index: 5 , elements: ['b']\",\n",
       " 'index: 6 , elements: []',\n",
       " \"index: 7 , elements: ['c']\"]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "RDD.mapPartitionsWithIndex(f: Callable[[int, Iterable[T]], Iterable[U]], \n",
    "preservesPartitioning: bool = False) → pyspark.rdd.RDD[U]\n",
    "'''\n",
    "print('mapPartitionsWithIndex ')\n",
    "a_coll_rdd.mapPartitionsWithIndex(show_partitions).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(sidx, itr): yield sidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_coll_rdd.mapPartitionsWithIndex(f).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we require a function to iterate over the elements of a partition\n",
    "def map_partitions_function(itera): \n",
    "    yield ', '.join(list(itera))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', 'a', '', '', 'b', '', 'c']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_coll_rdd.mapPartitions(map_partitions_function).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pair RDD Operations - Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Union of two rdds:\n",
      "\n",
      "union:  [1, 2, 3, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# RDD.union(other: pyspark.rdd.RDD[U]) → pyspark.rdd.RDD[Union[T, U]]\n",
    "print('Union of two rdds:\\n')\n",
    "xu = sc.parallelize([1, 2, 3], 2)\n",
    "yu = sc.parallelize([3, 4], 1)\n",
    "print(\"union: \" , xu.union(yu).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', (2, 5)), ('a', (1, 3)), ('a', (1, 4))]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "RDD.join(other: pyspark.rdd.RDD[Tuple[K, U]], numPartitions: Optional[int] = None) → \n",
    "pyspark.rdd.RDD[Tuple[K, Tuple[V, U]]][source]\n",
    "\n",
    "'''\n",
    "xj = sc.parallelize([(\"a\", 1), (\"b\", 2)])\n",
    "yj = sc.parallelize([(\"a\", 3), (\"a\", 4), (\"b\", 5)])\n",
    "print(xj.join(yj).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left Outer Join\n",
      "[('b', (2, 5)), ('c', (7, None)), ('a', (1, 3)), ('a', (1, 4))]\n",
      "Right Outer Join\n",
      "[('b', (2, 5)), ('a', (1, 3)), ('a', (1, 4)), ('d', (None, 4))]\n"
     ]
    }
   ],
   "source": [
    "print(\"Left Outer Join\")\n",
    "xoj = sc.parallelize([(\"a\", 1), (\"b\", 2), (\"c\", 7)])\n",
    "yoj = sc.parallelize([(\"a\", 3), (\"a\", 4), (\"b\", 5), (\"d\", 4)])\n",
    "print(xoj.leftOuterJoin(yoj).collect())\n",
    "print(\"Right Outer Join\")\n",
    "print(xoj.rightOuterJoin(yoj).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Outer Join\n",
      "[('b', (2, 5)), ('c', (7, None)), ('a', (1, 3)), ('a', (1, 4)), ('d', (None, 4))]\n"
     ]
    }
   ],
   "source": [
    "print(\"Full Outer Join\")\n",
    "print(xoj.fullOuterJoin(yoj).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct\n",
      "\n",
      "Randomly generated list with repeat elements  [4 3 2 4 2 2 4 2 4 2]\n",
      "[2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "print('Distinct\\n')\n",
    "rpt_list = np.random.randint(0, 5, 10)\n",
    "print('Randomly generated list with repeat elements ', rpt_list)\n",
    "rpt_rdd = sc.parallelize(rpt_list)\n",
    "print(rpt_rdd.distinct().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coalesce to reduce number of partitions\n",
      "4\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# RDD.coalesce(numPartitions: int, shuffle: bool = False) → pyspark.rdd.RDD[T][source]\n",
    "print(\"coalesce to reduce number of partitions\")\n",
    "xCoalesce = sc.parallelize(range(10), 4)\n",
    "print(xCoalesce.getNumPartitions())\n",
    "yCoalesce = xCoalesce.coalesce(2)\n",
    "print(yCoalesce.getNumPartitions())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyBy transformation\n",
      "[(True, 0), (False, 1), (True, 2), (False, 3), (True, 4), (False, 5), (True, 6), (False, 7), (True, 8), (False, 9)]\n",
      "[('even', 0), ('odd', 1), ('even', 2), ('odd', 3), ('even', 4), ('odd', 5), ('even', 6), ('odd', 7), ('even', 8), ('odd', 9)]\n"
     ]
    }
   ],
   "source": [
    "# RDD.keyBy(f: Callable[[T], K]) → pyspark.rdd.RDD[Tuple[K, T]]\n",
    "print('keyBy transformation')\n",
    "new_no_rdd = sc.parallelize(range(10))\n",
    "print(new_no_rdd.keyBy(lambda x: x % 2 == 0).collect())\n",
    "print(new_no_rdd.keyBy(lambda x: x % 2 == 0).map(lambda x: ('even' if x[0] else 'odd', x[1])).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Partitioner Spark RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_function(k):\n",
    "    return 0 if k < 'H' else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"index: 0 , elements: [('F', 'Fred'), ('A', 'Anna')]\",\n",
       " \"index: 1 , elements: [('J', 'James'), ('J', 'John')]\"]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_part = sc.parallelize([('J', \"James\"), ('F', \"Fred\"), ('A', \"Anna\"), ('J', \"John\")], 3)\n",
    "x_part.partitionBy(2, part_function).mapPartitionsWithIndex(show_partitions).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping RDDs\n",
      "[(0, 1), (1, 4), (2, 9), (3, 16)]\n"
     ]
    }
   ],
   "source": [
    "azip = sc.parallelize(range(4))\n",
    "bzip = sc.parallelize([1, 4, 9, 16])\n",
    "\n",
    "print(\"Zipping RDDs\")\n",
    "print(azip.zip(bzip).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 11, 3, 1, 1, 14, 12, 7, 14, 4]\n"
     ]
    }
   ],
   "source": [
    "rand_rdd = sc.parallelize(np.random.randint(0, 20, 10))\n",
    "print(rand_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple sort ascending\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 3, 4, 7, 11, 12, 14, 14]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD.sortBy(keyfunc: Callable[[T], S], ascending: bool = True, numPartitions: Optional[int] = None) → RDD[T]\n",
    "print('Simple sort ascending')\n",
    "rand_rdd.sortBy(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple sort descending\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[14, 14, 12, 11, 7, 4, 3, 1, 1, 0]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Simple sort descending')\n",
    "rand_rdd.sortBy(lambda x: -x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_rdd = sc.parallelize([(\"arjun\", \"tendulkar\", 5),\n",
    "    (\"sachin\", \"tendulkar\", 102), (\"vachin\", \"tendulkar\", 102),\n",
    "    (\"rahul\", \"dravid\", 74), (\"vahul\", \"dravid\", 74),\n",
    "    (\"rahul\", \"shavid\", 74), (\"vahul\", \"shavid\", 74),\n",
    "    (\"jacques\", \"kallis\", 92), (\"ricky\", \"ponting\", 84), (\"jacques\", \"zaalim\", 92),\n",
    "    (\"sachin\", \"vendulkar\", 102)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('jacques', 'zaalim', 92),\n",
       " ('sachin', 'vendulkar', 102),\n",
       " ('arjun', 'tendulkar', 5),\n",
       " ('sachin', 'tendulkar', 102),\n",
       " ('vachin', 'tendulkar', 102),\n",
       " ('rahul', 'shavid', 74),\n",
       " ('vahul', 'shavid', 74),\n",
       " ('ricky', 'ponting', 84),\n",
       " ('jacques', 'kallis', 92),\n",
       " ('rahul', 'dravid', 74),\n",
       " ('vahul', 'dravid', 74)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_rdd.sortBy(lambda x: x[1], False).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+\n",
      "|  fname|    lname|centuries|\n",
      "+-------+---------+---------+\n",
      "|  arjun|tendulkar|        5|\n",
      "| sachin|tendulkar|      102|\n",
      "| vachin|tendulkar|      102|\n",
      "|  rahul|   dravid|       74|\n",
      "|  vahul|   dravid|       74|\n",
      "|  rahul|   shavid|       74|\n",
      "|  vahul|   shavid|       74|\n",
      "|jacques|   kallis|       92|\n",
      "|  ricky|  ponting|       84|\n",
      "|jacques|   zaalim|       92|\n",
      "| sachin|vendulkar|      102|\n",
      "+-------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comp_df = comp_rdd.toDF(['fname', 'lname', 'centuries'])\n",
    "comp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+\n",
      "|  fname|    lname|centuries|\n",
      "+-------+---------+---------+\n",
      "|  vahul|   dravid|       74|\n",
      "|  rahul|   dravid|       74|\n",
      "|jacques|   kallis|       92|\n",
      "|  ricky|  ponting|       84|\n",
      "|  vahul|   shavid|       74|\n",
      "|  rahul|   shavid|       74|\n",
      "| vachin|tendulkar|      102|\n",
      "| sachin|tendulkar|      102|\n",
      "|  arjun|tendulkar|        5|\n",
      "| sachin|vendulkar|      102|\n",
      "|jacques|   zaalim|       92|\n",
      "+-------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "comp_df.sort('lname', desc('fname'), desc('centuries')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD Aggregation\n",
    "### A sequence operation which will run as a combiner of sort on the elements of the partition\n",
    "### A combining operation which will reduce the combined tuples from the partitions to a single tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First take a look at the partitions of the rdd\n",
      "['index: 0 , elements: [1, 2]', 'index: 1 , elements: [3, 4]', 'index: 2 , elements: [5, 6]', 'index: 3 , elements: [7, 8]', 'index: 4 , elements: [9, 10]', 'index: 5 , elements: [11, 12]']\n",
      "(78, 12)\n",
      "(78, 12)\n"
     ]
    }
   ],
   "source": [
    "# RDD.aggregate(zeroValue: U, seqOp: Callable[[U, T], U], combOp: Callable[[U, U], U]) → U\n",
    "def seq_op(data, elem):\n",
    "    return (data[0] + elem, data[1] + 1)\n",
    "\n",
    "def comb_op(d1, d2):\n",
    "    return (d1[0] + d2[0], d1[1] + d2[1])\n",
    "\n",
    "ardd = sc.parallelize(range(1, 13), 6)\n",
    "print('First take a look at the partitions of the rdd')\n",
    "print(ardd.mapPartitionsWithIndex(show_partitions).collect())\n",
    "print(ardd.aggregate((0,0), seq_op, comb_op))\n",
    "\n",
    "print(ardd.aggregate((0, 0), \n",
    "                    lambda data, elem: (data[0] + elem, data[1] + 1),\n",
    "                    lambda d1, d2: (d1[0] + d2[0], d1[1] + d2[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Tree aggregations operate exactly in the same way as aggreate  except for one critical difference - there is an intermediate aggregation step  data from some partitions will be sent to executors to aggregate\n",
    " ### so in the above case if there are six partitions  while aggregate will send results of all the six partitions to the driver in tree aggregate, three will go to one executor, three to another and the driver will receive the aggregations from 2 rather than 6\n",
    " ### Where there are many number of partitions tree aggregate performs significantly better than vanilla aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78, 12)\n"
     ]
    }
   ],
   "source": [
    "print(ardd.treeAggregate((0,0), seq_op, comb_op))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pow_2_to_4_rdd = sc.parallelize([(1, 1), (2, 4), (3, 9), (1, 1), (2, 8), (3, 27),\n",
    "      (1, 1), (2, 16), (3, 81), (4, 256)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (3, 3)), (2, (28, 3)), (3, (117, 3)), (4, (256, 1))]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow_2_to_4_rdd.combineByKey(lambda x: (x, 1),\n",
    "                lambda acc, vlu: (acc[0] + vlu, acc[1] + 1),  \n",
    "                lambda v1, v2: (v1[0] + v2[0], v1[1] + v2[1])).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aggregateByKey also will function at the partition level - we need a value to seed the aggregation and we have a combination of a sequence operation and a combo operation playing out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 3), (2, 28), (3, 117), (4, 256)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "RDD.aggregateByKey(zeroValue: U, seqFunc: Callable[[U, V], U], combFunc: Callable[[U, U], U], \n",
    "numPartitions: Optional[int] = None, partitionFunc: Callable[[K], int] = <function portable_hash>) →\n",
    "pyspark.rdd.RDD[Tuple[K, U]][source]\n",
    "'''\n",
    "pow_2_to_4_rdd.aggregateByKey(0,\n",
    "                   lambda x, y: x + y, \n",
    "                   lambda a, b: a + b\n",
    "                  ).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

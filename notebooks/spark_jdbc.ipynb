{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['SPARK_HOME']='/usr/lib/spark'\n",
    "os.environ['PYLIB']=os.environ['SPARK_HOME']+'/python/lib'\n",
    "sys.path.insert(0,os.environ['PYLIB']+'/py4j-0.10.7-src.zip')\n",
    "sys.path.insert(1,os.environ['PYLIB']+'/pyspark.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('TestHive') \\\n",
    ".config('spark.warehouse.dir','/apps/hive/warehouse') \\\n",
    ".config('spark.driver.extraClassPath', \n",
    "        '/home/cloudera/.m2/repository/mysql/mysql-connector-java/5.1.44/mysql-connector-java-5.1.44.jar') \\\n",
    ".config('spark.executor.extraClassPath', \n",
    "        '/home/cloudera/.m2/repository/mysql/mysql-connector-java/5.1.44/mysql-connector-java-5.1.44.jar') \\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Having set the driver and driver options we should have spark representing spark session \n",
    "# available straight away\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---+\n",
      "|   fname|   lname|age|\n",
      "+--------+--------+---+\n",
      "|    anil|  kapoor| 54|\n",
      "|  sanjay|    dutt| 56|\n",
      "|   arjun|  kappor| 28|\n",
      "|  ranbir|  kappor| 31|\n",
      "|  deepka|padukone| 29|\n",
      "|shahrukh|    khan| 51|\n",
      "| hrithik|  roshan| 37|\n",
      "+--------+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prop = {'user': 'root', 'password': 'cloudera', 'driver': 'com.mysql.jdbc.Driver'}\n",
    "url = 'jdbc:mysql://localhost:3306/testdb'\n",
    "mrdf = spark.read.format('jdbc').option('url', url).option('dbtable', 'simptable') \\\n",
    ".option('user', 'root').option('password', 'cloudera') \\\n",
    ".option('driver', 'com.mysql.jdbc.Driver').load()\n",
    "mrdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fname: string (nullable = true)\n",
      " |-- lname: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mrdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|   lname|count|\n",
      "+--------+-----+\n",
      "|padukone|    1|\n",
      "|  roshan|    1|\n",
      "|  kappor|    2|\n",
      "|    khan|    1|\n",
      "|  kapoor|    1|\n",
      "|    dutt|    1|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mrdf.groupBy('lname').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---+\n",
      "|   fname|   lname|age|\n",
      "+--------+--------+---+\n",
      "|    anil|  kapoor| 54|\n",
      "|  sanjay|    dutt| 56|\n",
      "|   arjun|  kappor| 28|\n",
      "|  ranbir|  kappor| 31|\n",
      "|  deepka|padukone| 29|\n",
      "|shahrukh|    khan| 51|\n",
      "| hrithik|  roshan| 37|\n",
      "+--------+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mrdf_age_lim = spark.read.format('jdbc').option('url', url).option('dbtable', 'simptable') \\\n",
    ".option('user', 'root').option('password', 'cloudera') \\\n",
    ".option('driver', 'com.mysql.jdbc.Driver') \\\n",
    ".option('partitionColumn', 'age').option('lowerBound', 10).option('upperBound', 30) \\\n",
    ".option('numPartitions', 2).load()\n",
    "mrdf_age_lim.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format('jdbc').option('url', url).option('dbtable', 'nstable') \\\n",
    ".option('user', 'root').option('password', 'cloudera') \\\n",
    ".option('driver', 'com.mysql.jdbc.Driver').load().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrdf.write.jdbc(url=url, mode='append', table='nstable', properties=prop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

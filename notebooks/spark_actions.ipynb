{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['SPARK_HOME']='D:/spark330hdp3sc3'\n",
    "os.environ['PYLIB']=os.environ['SPARK_HOME']+'/python/lib'\n",
    "sys.path.insert(0,os.environ['PYLIB']+'/py4j-0.10.9.5-src.zip')\n",
    "sys.path.insert(1,os.environ['PYLIB']+'/pyspark.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('TestHive') \\\n",
    ".config('spark.warehouse.dir','/apps/hive/warehouse') \\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Having set the driver and driver options we should have spark representing spark session \n",
    "# available straight away\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduce,  10\n",
      "collect,  [1, 2, 3, 4]\n",
      "keys,  [1, 2, 3, 4]\n",
      "values,  [1, 2, 3, 4]\n",
      "aggregate,  (4, 10)\n",
      "first,  1\n",
      "take,  [1, 2]\n",
      "top plain ,  [200, 21]\n",
      "top with function,  [2, 5]\n",
      "sample,  [1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 4, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "ardd = sc.parallelize(range(1, 5))\n",
    "pair_rdd = ardd.map(lambda x: (x, x))\n",
    "# RDD.reduce(f: Callable[[T, T], T]) → T\n",
    "print('reduce, ', ardd.reduce(lambda x, y: x + y))\n",
    "# RDD.collect() → List[T][source]\n",
    "print('collect, ', ardd.collect())\n",
    "print('keys, ', pair_rdd.keys().collect())\n",
    "print('values, ', pair_rdd.values().collect())\n",
    "# RDD.aggregate(zeroValue: U, seqOp: Callable[[U, T], U], combOp: Callable[[U, U], U]) → U\n",
    "print('aggregate, ', ardd.aggregate((0, 0), lambda acc, vlu: (acc[0] + 1, acc[1] + vlu),\n",
    "                                  lambda x, y: (x[0] + y[0], x[1] + y[1])))\n",
    "# RDD.first() → T\n",
    "print('first, ' , ardd.first())\n",
    "# RDD.take(num: int) → List[T]                      \n",
    "print('take, ', ardd.take(2))\n",
    "# RDD.top(num: int, key: Optional[Callable[[T], S]] = None) → List[T] \n",
    "print('top plain , ', sc.parallelize([10, 17, 21, 200, 2, 14, 16, 5]).top(2))\n",
    "print('top with function, ', sc.parallelize([10, 17, 21, 200, 2, 14, 16, 5]).top(2, lambda x: -x))\n",
    "# RDD.sample(withReplacement: bool, fraction: float, seed: Optional[int] = None) → pyspark.rdd.RDD[T]\n",
    "print('sample, ', ardd.sample(True, 2, 5).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collect as map,  {1: 1, 2: 2, 3: 3, 4: 4}\n"
     ]
    }
   ],
   "source": [
    "# RDD.collectAsMap() → Dict[K, V]\n",
    "print('collect as map, ', pair_rdd.collectAsMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count by key  defaultdict(<class 'int'>, {1: 1, 2: 2, 3: 3, 4: 4})\n",
      "count by value  defaultdict(<class 'int'>, {(1, 1): 1, (2, 2): 2, (3, 3): 3, (4, 4): 4})\n"
     ]
    }
   ],
   "source": [
    "# RDD.countByKey() → Dict[K, int]\n",
    "print('count by key ', ardd.flatMap(lambda x: range(x, 5)).map(lambda x: (x, x)).countByKey())\n",
    "# RDD.countByValue() → Dict[K, int]\n",
    "print('count by value ', ardd.flatMap(lambda x: range(x, 5)).map(lambda x: (x, x)).countByValue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min:  1 , Max:  4  Sum:  10\n",
      "Mean:  2.5 , StDev:  1.118033988749895 , Variance:  1.25\n",
      "All together in stats:  (count: 4, mean: 2.5, stdev: 1.118033988749895, max: 4.0, min: 1.0)\n"
     ]
    }
   ],
   "source": [
    "# RDD.min(key: Optional[Callable[[T], S]] = None) → T\n",
    "# RDD.max(key: Optional[Callable[[T], S]] = None) → T\n",
    "# RDD.mean() → float\n",
    "print(\"Min: \" , ardd.min() , \", Max: \" , ardd.max() , \" Sum: \" , ardd.sum())\n",
    "print(\"Mean: \" , ardd.mean() , \", StDev: \" , ardd.stdev() , \", Variance: \" ,\n",
    "    ardd.variance())\n",
    "# RDD.stats() → pyspark.statcounter.StatCounter\n",
    "print(\"All together in stats: \" , ardd.stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[75 13 90 88 85 83 84 27 48 45 52 65 19 48 21 60 51 86 50 83  2  0  8 97\n",
      " 72 43 41 81 56 57]\n",
      "buckets and frequency of the histogram generated:  [(0.0, 7), (32.333333333333336, 11), (64.66666666666667, 12)]\n"
     ]
    }
   ],
   "source": [
    "randlist = np.random.randint(0, 100, 30)\n",
    "print(randlist)\n",
    "randRDD = sc.parallelize(randlist)\n",
    "# RDD.histogram(buckets: Union[int, List[S], Tuple[S, …]]) → Tuple[Sequence[S], List[int]]\n",
    "histogram = randRDD.histogram(3)\n",
    "print('buckets and frequency of the histogram generated: ', [x for x in zip(histogram[0], histogram[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([2, 5, 10, 20], [2, 1, 13])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_list = [1, 3, 10, 11, 12, 2, 9,12, 12, 12, 12, 12, 12, 12,12,12, 14]\n",
    "print(len(new_list))\n",
    "sc.parallelize(new_list).histogram([2, 5, 10, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "countApprox:  100000000\n"
     ]
    }
   ],
   "source": [
    "# RDD.countApprox(timeout: int, confidence: float = 0.95) → int\n",
    "approxRDD = sc.parallelize(range(100000000)).map(lambda x: ('even' if x % 2 == 0 else 'odd', x))\n",
    "print(\"countApprox: \" , approxRDD.countApprox(100, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving pairRDD to text file - directory will be created with one file for each partition\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSaving pairRDD to text file - directory will be created with one file for each partition\")\n",
    "save_file_location = 'D:/tmp/prdd_text_py'\n",
    "pair_rdd.saveAsTextFile(save_file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from the saved text file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['(1, 1)', '(2, 2)', '(3, 3)', '(4, 4)']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD.saveAsTextFile(path: str, compressionCodecClass: Optional[str] = None) → None\n",
    "print(\"Loading from the saved text file\")\n",
    "sc.textFile(save_file_location).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(k=1, v=1), Row(k=2, v=2), Row(k=3, v=3), Row(k=4, v=4)]\n",
      "reading from stored parquet file  [Row(k=1, v=1), Row(k=2, v=2), Row(k=3, v=3), Row(k=4, v=4)]\n"
     ]
    }
   ],
   "source": [
    "pairDF = pair_rdd.toDF([\"k\", \"v\"])\n",
    "pair\n",
    "print(pairDF.collect())\n",
    "pairDF.write.parquet(\"hdfs://localhost:8020/user/cloudera/prdd_parquet_py\")\n",
    "print('reading from stored parquet file ',\n",
    "      spark.read.parquet(\"hdfs://localhost:8020/user/cloudera/prdd_parquet_py\").collect())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

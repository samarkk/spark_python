{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['SPARK_HOME']='/usr/lib/spark'\n",
    "os.environ['PYLIB']=os.environ['SPARK_HOME']+'/python/lib'\n",
    "sys.path.insert(0,os.environ['PYLIB']+'/py4j-0.10.7-src.zip')\n",
    "sys.path.insert(1,os.environ['PYLIB']+'/pyspark.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('TestHive') \\\n",
    ".config('spark.warehouse.dir','/apps/hive/warehouse') \\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Having set the driver and driver options we should have spark representing spark session \n",
    "# available straight away\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduce,  10\n",
      "collect,  [1, 2, 3, 4]\n",
      "keys,  [1, 2, 3, 4]\n",
      "values,  [1, 2, 3, 4]\n",
      "aggregate,  (4, 10)\n",
      "first,  1\n",
      "take,  [1, 2]\n",
      "top,  [4, 3]\n",
      "samole,  [1, 1, 2, 2, 3, 3, 3, 4, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "ardd = sc.parallelize(range(1, 5))\n",
    "pair_rdd = ardd.map(lambda x: (x, x))\n",
    "print('reduce, ', ardd.reduce(lambda x, y: x + y))\n",
    "print('collect, ', ardd.collect())\n",
    "print('keys, ', pair_rdd.keys().collect())\n",
    "print('values, ', pair_rdd.values().collect())\n",
    "print('aggregate, ', ardd.aggregate((0, 0), lambda acc, vlu: (acc[0] + 1, acc[1] + vlu),\n",
    "                                  lambda x, y: (x[0] + y[0], x[1] + y[1])))\n",
    "print('first, ' , ardd.first())\n",
    "print('take, ', ardd.take(2))\n",
    "print('top, ', ardd.sortBy(lambda x: x, False).top(2))\n",
    "# def sample(withReplacement: Boolean, fraction: Double, seed: Int): RDD[T]\n",
    "print('sample, ', ardd.sample(True, 2, 5).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collect as map,  {1: 1, 2: 2, 3: 3, 4: 4}\n"
     ]
    }
   ],
   "source": [
    "print('collect as map, ', pair_rdd.collectAsMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count by key  defaultdict(<class 'int'>, {1: 1, 2: 2, 3: 3, 4: 4})\n",
      "count by value  defaultdict(<class 'int'>, {(1, 1): 1, (2, 2): 2, (3, 3): 3, (4, 4): 4})\n"
     ]
    }
   ],
   "source": [
    "print('count by key ', ardd.flatMap(lambda x: range(x, 5)).map(lambda x: (x, x)).countByKey())\n",
    "print('count by value ', ardd.flatMap(lambda x: range(x, 5)).map(lambda x: (x, x)).countByValue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "arddDbl = sc.parallelize([float(x) for x in range(1, 5)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min:  1 , Max:  4  Sum:  10\n",
      "Mean:  2.5 , StDev:  1.118033988749895 , Variance:  1.25\n",
      "All together in stats:  (count: 4, mean: 2.5, stdev: 1.118033988749895, max: 4.0, min: 1.0)\n"
     ]
    }
   ],
   "source": [
    "print(\"Min: \" , ardd.min() , \", Max: \" , ardd.max() , \" Sum: \" , ardd.sum())\n",
    "print(\"Mean: \" , arddDbl.mean() , \", StDev: \" , arddDbl.stdev() , \", Variance: \" ,\n",
    "    arddDbl.variance())\n",
    "print(\"All together in stats: \" , arddDbl.stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buckets and frequency of the histogram generated:  [(0.0, 14), (9.9, 8), (19.8, 14), (29.700000000000003, 7), (39.6, 8), (49.5, 5), (59.400000000000006, 14), (69.3, 10), (79.2, 9), (89.10000000000001, 11)]\n"
     ]
    }
   ],
   "source": [
    "randlist = np.random.randint(0, 100, 100)\n",
    "randRDD = sc.parallelize(randlist)\n",
    "histogram = randRDD.histogram(10)\n",
    "print('buckets and frequency of the histogram generated: ', [x for x in zip(histogram[0], histogram[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "countApprox:  100000000\n"
     ]
    }
   ],
   "source": [
    "approxRDD = sc.parallelize(range(100000000)).map(lambda x: ('even' if x % 2 == 0 else 'odd', x))\n",
    "print(\"countApprox: \" , approxRDD.countApprox(100, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving pairRDD to text file - directory will be created with one file for each partition\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSaving pairRDD to text file - directory will be created with one file for each partition\")\n",
    "pair_rdd.saveAsTextFile(\"hdfs://localhost:8020/user/cloudera/prdd_text_py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from the saved text file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['(1, 1)', '(2, 2)', '(3, 3)', '(4, 4)']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading from the saved text file\")\n",
    "sc.textFile(\"hdfs://localhost:8020/user/cloudera/prdd_text_py\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdp_rdd = sc.newAPIHadoopFile(\"hdfs://localhost:8020/user/cloudera/prdd_text_py\",\n",
    "    'org.apache.hadoop.mapreduce.lib.input.TextInputFormat', 'org.apache.hadoop.io.LongWritable',\n",
    "    'org.apache.hadoop.io.Text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1), (2, 2), (3, 3), (4, 4)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, '(1, 1)'), (7, '(2, 2)'), (0, '(3, 3)'), (7, '(4, 4)')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(pair_rdd.collect())\n",
    "hdp_rdd.map(lambda x: (x[0], x[1])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving pairRDD to object file - directory will be created with one file for each partition\n",
      "Loading from the saved sequence file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 1), (2, 2), (3, 3), (4, 4)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nSaving pairRDD to object file - directory will be created with one file for each partition\")\n",
    "pair_rdd.saveAsSequenceFile(\"hdfs://localhost:8020/user/cloudera/prdd_sequence_py\")\n",
    "print(\"Loading from the saved sequence file\")\n",
    "sc.sequenceFile(\"hdfs://localhost:8020/user/cloudera/prdd_sequence_py\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(k=1, v=1), Row(k=2, v=2), Row(k=3, v=3), Row(k=4, v=4)]\n",
      "reading from stored parquet file  [Row(k=1, v=1), Row(k=2, v=2), Row(k=3, v=3), Row(k=4, v=4)]\n"
     ]
    }
   ],
   "source": [
    "pairDF = pair_rdd.toDF([\"k\", \"v\"])\n",
    "print(pairDF.collect())\n",
    "pairDF.write.parquet(\"hdfs://localhost:8020/user/cloudera/prdd_parquet_py\")\n",
    "print('reading from stored parquet file ',\n",
    "      spark.read.parquet(\"hdfs://localhost:8020/user/cloudera/prdd_parquet_py\").collect())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

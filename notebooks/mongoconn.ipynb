{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['SPARK_HOME']='/home/cloudera/spark230hadoop26'\n",
    "os.environ['PYLIB']=os.environ['SPARK_HOME']+'/python/lib'\n",
    "sys.path.insert(0,os.environ['PYLIB']+'/py4j-0.10.6-src.zip')\n",
    "sys.path.insert(1,os.environ['PYLIB']+'/pyspark.zip')\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.mongodb.spark:mongo-spark-connector_2.11:2.2.3 pyspark-shell'\n",
    "os.environ['PYSPARK_PYTHON'] = '/home/cloudera/anaconda3/bin/python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('TestHive') \\\n",
    ".config('spark.warehouse.dir','/apps/hive/warehouse') \\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Having set the driver and driver options we should have spark representing spark session \n",
    "# available straight away\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = spark.createDataFrame([(\"Bilbo Baggins\",  50), (\"Gandalf\", 1000), (\"Thorin\", 195), \n",
    "    (\"Balin\", 178), (\"Kili\", 77), (\"Dwalin\", 169), (\"Oin\", 167), \n",
    "    (\"Gloin\", 158), (\"Fili\", 82), (\"Bombur\", None)], [\"name\", \"age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+\n",
      "|         name| age|\n",
      "+-------------+----+\n",
      "|Bilbo Baggins|  50|\n",
      "|      Gandalf|1000|\n",
      "+-------------+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.write.format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    ".mode(\"append\").option('spark.mongodb.output.uri', 'mongodb://127.0.0.1:27017').option(\"database\",\n",
    "\"people\").option(\"collection\", \"contacts\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\",\n",
    "\"mongodb://127.0.0.1/people.contacts\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-------------+\n",
      "|                 _id|   age|         name|\n",
      "+--------------------+------+-------------+\n",
      "|[5b3748d6ffc0ca4a...| 169.0|       Dwalin|\n",
      "|[5b3748d6ffc0ca4a...| 167.0|          Oin|\n",
      "|[5b3748d6ffc0ca4a...| 158.0|        Gloin|\n",
      "|[5b3748d6ffc0ca4a...|  82.0|         Fili|\n",
      "|[5b3748d6ffc0ca4a...|  null|       Bombur|\n",
      "|[5b3748d6ffc0ca4a...|  50.0|Bilbo Baggins|\n",
      "|[5b3748d6ffc0ca4a...|1000.0|      Gandalf|\n",
      "|[5b3748d6ffc0ca4a...| 195.0|       Thorin|\n",
      "|[5b3748d6ffc0ca4a...| 178.0|        Balin|\n",
      "|[5b3748d6ffc0ca4a...|  77.0|         Kili|\n",
      "|[5b3749dc361d6856...|  27.0|    test_name|\n",
      "|[5b375b0dffc0ca4e...|  50.0|Bilbo Baggins|\n",
      "|[5b375b0dffc0ca4e...|1000.0|      Gandalf|\n",
      "|[5b375b0dffc0ca4e...| 195.0|       Thorin|\n",
      "|[5b375b0dffc0ca4e...| 178.0|        Balin|\n",
      "|[5b375b0dffc0ca4e...|  77.0|         Kili|\n",
      "|[5b375b0dffc0ca4e...| 169.0|       Dwalin|\n",
      "|[5b375b0dffc0ca4e...| 167.0|          Oin|\n",
      "|[5b375b0dffc0ca4e...| 158.0|        Gloin|\n",
      "|[5b375b0dffc0ca4e...|  82.0|         Fili|\n",
      "+--------------------+------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+\n",
      "|_id|qty| type|\n",
      "+---+---+-----+\n",
      "|1.0|5.0|apple|\n",
      "+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# simple pipeline operating against the fruit collection which will just carry out a match\n",
    "# and deliver back ther results\n",
    "pipeline = \"{'$match': {'type': 'apple'}}\"\n",
    "df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"pipeline\", pipeline) \\\n",
    ".option('spark.mongodb.input.uri', 'mongodb://localhost:27017').option('database', 'people') \\\n",
    ".option('collection', 'fruit').load()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the zips collection group by state and get the population sum\n",
    "simple_zip_pipeline = '''{ '$group': { '_id' : '$state', totalPop: { $sum: '$pop' } } }'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "|_id|totalPop|\n",
      "+---+--------+\n",
      "| CA|29754890|\n",
      "| MT|  798948|\n",
      "| MS| 2573216|\n",
      "| FL|12686644|\n",
      "| AR| 2350725|\n",
      "| GA| 6478216|\n",
      "| WA| 4866692|\n",
      "| SC| 3486703|\n",
      "| MN| 4372982|\n",
      "| NE| 1578139|\n",
      "| MD| 4781379|\n",
      "| TN| 4876457|\n",
      "| DE|  666168|\n",
      "| DC|  606900|\n",
      "| AZ| 3665228|\n",
      "| ME| 1226648|\n",
      "| OR| 2842321|\n",
      "| AL| 4040587|\n",
      "| PA|11881643|\n",
      "| RI| 1003218|\n",
      "+---+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simple_zip_df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"pipeline\", simple_zip_pipeline) \\\n",
    ".option('spark.mongodb.input.uri', 'mongodb://localhost:27017').option('database', 'people') \\\n",
    ".option('collection', 'zipcodes').load()\n",
    "simple_zip_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frofm the zips collection after aggregating population for states find those with pop > 10 mn\n",
    "zip_pipeline = '''[{ '$group': { '_id' : '$state', totalPop: { $sum: '$pop' } } }, \n",
    " { '$match': { totalPop: { $gte: 10000000 } }}]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "|_id|totalPop|\n",
      "+---+--------+\n",
      "| CA|29754890|\n",
      "| FL|12686644|\n",
      "| PA|11881643|\n",
      "| NY|17990402|\n",
      "| OH|10846517|\n",
      "| IL|11427576|\n",
      "| TX|16984601|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zip_df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"pipeline\", zip_pipeline) \\\n",
    ".option('spark.mongodb.input.uri', 'mongodb://localhost:27017').option('database', 'people') \\\n",
    ".option('collection', 'zipcodes').load()\n",
    "zip_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_city_pop_pipeline = '''[{ $group: { _id: { state: \"$state\", city: \"$city\" }, pop: { $sum: \"$pop\" } } },\n",
    "   { $group: { _id: \"$_id.state\", nocities: {$sum: 1}, avgCityPop: { $avg: \"$pop\" } } },\n",
    "   { $sort: {avgCityPop: -1} }\n",
    "]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+--------+\n",
      "|_id|        avgCityPop|nocities|\n",
      "+---+------------------+--------+\n",
      "| DC|          303450.0|       2|\n",
      "| CA| 27756.42723880597|    1072|\n",
      "| FL|27400.958963282937|     463|\n",
      "| AZ| 20591.16853932584|     178|\n",
      "| RI|19292.653846153848|      52|\n",
      "| NV|18209.590909090908|      66|\n",
      "| HI|15831.842857142858|      70|\n",
      "| NJ| 15775.89387755102|     490|\n",
      "| MA| 14855.37037037037|     405|\n",
      "| CT|         14674.625|     224|\n",
      "| DE| 14481.91304347826|      46|\n",
      "| TX| 13775.02108678021|    1233|\n",
      "| NY|13131.680291970803|    1370|\n",
      "| OH|12700.839578454332|     854|\n",
      "| MD|12615.775725593667|     379|\n",
      "| WA|12258.670025188916|     397|\n",
      "| MI|12087.512353706112|     769|\n",
      "| GA| 11547.62210338681|     561|\n",
      "| SC|11139.626198083068|     313|\n",
      "| NC|10622.815705128205|     624|\n",
      "+---+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_city_df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"pipeline\", avg_city_pop_pipeline) \\\n",
    ".option('spark.mongodb.input.uri', 'mongodb://localhost:27017').option('database', 'people') \\\n",
    ".option('collection', 'zipcodes').load()\n",
    "avg_city_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_sql = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\",\n",
    "\"mongodb://127.0.0.1/people.zipcodes\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_sql.createOrReplaceTempView('ziptbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+--------------------+-----+-----+\n",
      "|  _id|       city|                 loc|  pop|state|\n",
      "+-----+-----------+--------------------+-----+-----+\n",
      "|01001|     AGAWAM|[-72.622739, 42.0...|15338|   MA|\n",
      "|01002|    CUSHMAN|[-72.51565, 42.37...|36963|   MA|\n",
      "|01005|      BARRE|[-72.108354, 42.4...| 4546|   MA|\n",
      "|01007|BELCHERTOWN|[-72.410953, 42.2...|10579|   MA|\n",
      "|01008|  BLANDFORD|[-72.936114, 42.1...| 1240|   MA|\n",
      "+-----+-----------+--------------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from ziptbl limit 5').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+------+\n",
      "|state|           city|avgpop|\n",
      "+-----+---------------+------+\n",
      "|   MA|   CHESTERFIELD|   177|\n",
      "|   NH|          GONIC|  4474|\n",
      "|   ME|           SACO| 16192|\n",
      "|   VT|UNIV OF VERMONT|     0|\n",
      "|   NJ|       PATERSON|141382|\n",
      "|   NJ|  THREE BRIDGES|   378|\n",
      "|   NY|      ROSENDALE|  2939|\n",
      "|   NY|      CONSTABLE|  1949|\n",
      "|   NY|     GEORGETOWN|   611|\n",
      "|   NY|      CLAYVILLE|   641|\n",
      "|   NY|          EATON|  1583|\n",
      "|   NY|    OSWEGATCHIE|   287|\n",
      "|   NY|   GREAT VALLEY|  2217|\n",
      "|   NY|       REXVILLE|   539|\n",
      "|   PA|      LAMBERTON|  4703|\n",
      "|   PA|     GRAPEVILLE|   683|\n",
      "|   PA| NEW ENTERPRISE|  1898|\n",
      "|   PA| ST CLAIRSVILLE|   174|\n",
      "|   PA|       NEW PARK|  1190|\n",
      "|   PA| NORTHUMBERLAND|  7326|\n",
      "+-----+---------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select state, city, sum(pop) as avgpop from ziptbl group by state, city').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------------------+\n",
      "|state|nocities|        avgcitypop|\n",
      "+-----+--------+------------------+\n",
      "|   DC|       2|          303450.0|\n",
      "|   CA|    1072| 27756.42723880597|\n",
      "|   FL|     463|27400.958963282937|\n",
      "|   AZ|     178| 20591.16853932584|\n",
      "|   RI|      52|19292.653846153848|\n",
      "|   NV|      66|18209.590909090908|\n",
      "|   HI|      70|15831.842857142858|\n",
      "|   NJ|     490| 15775.89387755102|\n",
      "|   MA|     405| 14855.37037037037|\n",
      "|   CT|     224|         14674.625|\n",
      "|   DE|      46| 14481.91304347826|\n",
      "|   TX|    1233| 13775.02108678021|\n",
      "|   NY|    1370|13131.680291970803|\n",
      "|   OH|     854|12700.839578454332|\n",
      "|   MD|     379|12615.775725593667|\n",
      "|   WA|     397|12258.670025188916|\n",
      "|   MI|     769|12087.512353706112|\n",
      "|   GA|     561| 11547.62210338681|\n",
      "|   SC|     313|11139.626198083068|\n",
      "|   NC|     624|10622.815705128205|\n",
      "+-----+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''select state, count(avgpop) as nocities, avg(avgpop) as avgcitypop from \n",
    "            ( select state, city, sum(pop) as avgpop from ziptbl group by state, city) as f\n",
    "            group by state order by avgcitypop desc''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "lscityzip = '''[\n",
    "   { $group:\n",
    "      {\n",
    "        _id: { state: \"$state\", city: \"$city\" },\n",
    "        pop: { $sum: \"$pop\" }\n",
    "      }\n",
    "   },\n",
    "   { $sort: { pop: 1 } },\n",
    "   { $group:\n",
    "      {\n",
    "        _id : \"$_id.state\",\n",
    "        biggestCity:  { $last: \"$_id.city\" },\n",
    "        biggestPop:   { $last: \"$pop\" },\n",
    "        smallestCity: { $first: \"$_id.city\" },\n",
    "        smallestPop:  { $first: \"$pop\" }\n",
    "      }\n",
    "   },\n",
    "  { $project:\n",
    "    { _id: 0,\n",
    "      state: \"$_id\",\n",
    "      biggestCity:  { name: \"$biggestCity\",  pop: \"$biggestPop\" },\n",
    "      smallestCity: { name: \"$smallestCity\", pop: \"$smallestPop\" }\n",
    "    }\n",
    "  }\n",
    "]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+\n",
      "|         biggestCity|        smallestCity|state|\n",
      "+--------------------+--------------------+-----+\n",
      "|    [NEWARK, 111674]|       [BETHEL, 108]|   DE|\n",
      "|   [JACKSON, 204788]|        [CHUNKY, 79]|   MS|\n",
      "|  [CRANSTON, 176404]|     [CLAYVILLE, 45]|   RI|\n",
      "|[SAINT LOUIS, 397...|      [BENDAVIS, 44]|   MO|\n",
      "|     [MIAMI, 825232]|[CECIL FIELD NAS, 0]|   FL|\n",
      "|[LITTLE ROCK, 192...|         [TOMATO, 0]|   AR|\n",
      "|   [ATLANTA, 609591]|   [FORT STEWART, 0]|   GA|\n",
      "| [BURLINGTON, 39127]|[UNIV OF VERMONT, 0]|   VT|\n",
      "|[ALBUQUERQUE, 449...|       [MONUMENT, 0]|   NM|\n",
      "|[PHILADELPHIA, 16...|       [HAMILTON, 0]|   PA|\n",
      "|   [WICHITA, 295115]|         [ARNOLD, 0]|   KS|\n",
      "|[MINNEAPOLIS, 344...|       [JOHNSON, 12]|   MN|\n",
      "|[LOS ANGELES, 210...|   [OREGON HOUSE, 0]|   CA|\n",
      "|   [BILLINGS, 78805]|      [HOMESTEAD, 7]|   MT|\n",
      "|[BIRMINGHAM, 242606]|          [ALLEN, 0]|   AL|\n",
      "|  [PORTLAND, 518543]|           [KENT, 0]|   OR|\n",
      "| [BALTIMORE, 733081]|[ANNAPOLIS JUNCTI...|   MD|\n",
      "|    [DENVER, 451182]|[CHEYENNE MTN AFB...|   CO|\n",
      "|   [PORTLAND, 63268]| [BUSTINS ISLAND, 0]|   ME|\n",
      "| [WORCESTER, 169856]|      [BUCKLAND, 16]|   MA|\n",
      "+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ls_city_df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"pipeline\", lscityzip) \\\n",
    ".option('spark.mongodb.input.uri', 'mongodb://localhost:27017').option('database', 'people') \\\n",
    ".option('collection', 'zipcodes').load()\n",
    "ls_city_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

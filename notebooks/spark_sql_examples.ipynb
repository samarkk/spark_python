{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['SPARK_HOME']='/home/cloudera/spark230hadoop26'\n",
    "os.environ['PYLIB']=os.environ['SPARK_HOME']+'/python/lib'\n",
    "sys.path.insert(0,os.environ['PYLIB']+'/py4j-0.10.6-src.zip')\n",
    "sys.path.insert(1,os.environ['PYLIB']+'/pyspark.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('TestHive') \\\n",
    ".config('spark.warehouse.dir','/apps/hive/warehouse') \\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Having set the driver and driver options we should have spark representing spark session \n",
    "# available straight away\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "acTransList = [\"SB10001,1000\", \"SB10002,1200\", \"SB10003,8000\",\n",
    "    \"SB10004,400\", \"SB10005,300\", \"SB10006,10000\", \"SB10007,500\",\n",
    "    \"SB10008,56\", \"SB10009,30\", \"SB10010,7000\",\n",
    "    \"CR10001,7000\", \"SB10002,-10\"]\n",
    "acRDD = sc.parallelize(acTransList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|  accNo|tranAmount|\n",
      "+-------+----------+\n",
      "|SB10001|      1000|\n",
      "|SB10002|      1200|\n",
      "|SB10003|      8000|\n",
      "|SB10004|       400|\n",
      "|SB10005|       300|\n",
      "|SB10006|     10000|\n",
      "|SB10007|       500|\n",
      "|SB10008|        56|\n",
      "|SB10009|        30|\n",
      "|SB10010|      7000|\n",
      "|CR10001|      7000|\n",
      "|SB10002|       -10|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acTransDF = acRDD.map(lambda x: x.split(',')).toDF(['accNo', 'tranAmount'])\n",
    "acTransDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- accno: string (nullable = true)\n",
      " |-- tranamount: string (nullable = true)\n",
      "\n",
      "None\n",
      "+-------+----------+\n",
      "|  accno|tranamount|\n",
      "+-------+----------+\n",
      "|SB10001|      1000|\n",
      "|SB10002|      1200|\n",
      "|SB10003|      8000|\n",
      "|SB10004|       400|\n",
      "|SB10005|       300|\n",
      "|SB10006|     10000|\n",
      "|SB10007|       500|\n",
      "|SB10008|        56|\n",
      "|SB10009|        30|\n",
      "|SB10010|      7000|\n",
      "|CR10001|      7000|\n",
      "|SB10002|       -10|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using create data frame method of spark\n",
    "from pyspark.sql.functions import *\n",
    "acTransDFFmSQLC = spark.createDataFrame(acRDD.map(lambda x: x.split(','))).toDF('accno', 'tranamount')\n",
    "print(acTransDFFmSQLC.printSchema())\n",
    "acTransDFFmSQLC.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data frame select column, columns\n",
      "[Row(accNo='SB10001'), Row(accNo='SB10002'), Row(accNo='SB10003'), Row(accNo='SB10004'), Row(accNo='SB10005'), Row(accNo='SB10006'), Row(accNo='SB10007'), Row(accNo='SB10008'), Row(accNo='SB10009'), Row(accNo='SB10010'), Row(accNo='CR10001'), Row(accNo='SB10002')]\n",
      "+-------+----------+\n",
      "|  accNo|tranAmount|\n",
      "+-------+----------+\n",
      "|SB10001|      1000|\n",
      "|SB10002|      1200|\n",
      "|SB10003|      8000|\n",
      "|SB10004|       400|\n",
      "|SB10005|       300|\n",
      "|SB10006|     10000|\n",
      "|SB10007|       500|\n",
      "|SB10008|        56|\n",
      "|SB10009|        30|\n",
      "|SB10010|      7000|\n",
      "|CR10001|      7000|\n",
      "|SB10002|       -10|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nData frame select column, columns\")\n",
    "# def select(col: String, cols: String*): DataFrame\n",
    "print(acTransDF.select(\"accNo\").collect())\n",
    "acTransDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fame selection using select expr\n",
      "+----------+----------+\n",
      "|account_no|tranAmount|\n",
      "+----------+----------+\n",
      "|   SB10001|      1000|\n",
      "|   SB10002|      1200|\n",
      "|   SB10003|      8000|\n",
      "|   SB10004|       400|\n",
      "|   SB10005|       300|\n",
      "|   SB10006|     10000|\n",
      "|   SB10007|       500|\n",
      "|   SB10008|        56|\n",
      "|   SB10009|        30|\n",
      "|   SB10010|      7000|\n",
      "|   CR10001|      7000|\n",
      "|   SB10002|       -10|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Data fame selection using select expr\")\n",
    "acTransDF.selectExpr(\"accNo as account_no\", \"tranAmount\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter equivalent to where, operation - filter and where seem to be equivalent\n",
      "+-------+----------+\n",
      "|  accNo|tranAmount|\n",
      "+-------+----------+\n",
      "|SB10001|      1000|\n",
      "|SB10002|      1200|\n",
      "|SB10003|      8000|\n",
      "|SB10006|     10000|\n",
      "|SB10010|      7000|\n",
      "|CR10001|      7000|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"filter equivalent to where, operation - filter and where seem to be equivalent\")\n",
    "acTransDF.filter(\"tranAmount >= 1000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|  accNo|tranAmount|\n",
      "+-------+----------+\n",
      "|SB10001|      1000|\n",
      "|SB10002|      1200|\n",
      "|SB10003|      8000|\n",
      "|SB10006|     10000|\n",
      "|SB10010|      7000|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acTransDF.where(\"tranAmount >= 1000 and accNo like'SB%' \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for conditions on multiple columns\n",
      "+-------+----------+\n",
      "|  accNo|tranAmount|\n",
      "+-------+----------+\n",
      "|SB10001|      1000|\n",
      "|SB10002|      1200|\n",
      "|SB10003|      8000|\n",
      "|SB10006|     10000|\n",
      "|SB10010|      7000|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "print(\"Filtering for conditions on multiple columns\")\n",
    "acTransDF.filter((acTransDF.tranAmount.cast('float') >= 1000) &\n",
    "                 (acTransDF.accNo.startswith('SB'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------+\n",
      "|  accNo|tranAmount| tranM10|\n",
      "+-------+----------+--------+\n",
      "|SB10001|      1000| 10000.0|\n",
      "|SB10002|      1200| 12000.0|\n",
      "|SB10003|      8000| 80000.0|\n",
      "|SB10004|       400|  4000.0|\n",
      "|SB10005|       300|  3000.0|\n",
      "|SB10006|     10000|100000.0|\n",
      "|SB10007|       500|  5000.0|\n",
      "|SB10008|        56|   560.0|\n",
      "|SB10009|        30|   300.0|\n",
      "|SB10010|      7000| 70000.0|\n",
      "|CR10001|      7000| 70000.0|\n",
      "|SB10002|       -10|  -100.0|\n",
      "+-------+----------+--------+\n",
      "\n",
      "+-------+----------+--------+\n",
      "|  accNo|tranAmount| tranM10|\n",
      "+-------+----------+--------+\n",
      "|SB10001|      1000| 10000.0|\n",
      "|SB10002|      1200| 12000.0|\n",
      "|SB10003|      8000| 80000.0|\n",
      "|SB10004|       400|  4000.0|\n",
      "|SB10005|       300|  3000.0|\n",
      "|SB10006|     10000|100000.0|\n",
      "|SB10007|       500|  5000.0|\n",
      "|SB10008|        56|   560.0|\n",
      "|SB10009|        30|   300.0|\n",
      "|SB10010|      7000| 70000.0|\n",
      "|CR10001|      7000| 70000.0|\n",
      "|SB10002|       -10|  -100.0|\n",
      "+-------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tenf = lambda x: x * 10\n",
    "tenudf = udf(tenf)\n",
    "acTransDF.select('accNo','tranAmount', tenudf(acTransDF.tranAmount.cast('float')).alias('tranM10')).show()\n",
    "acTransDF.select('accNo','tranAmount', tenudf(acTransDF['tranAmount'].cast('float')).alias('tranM10')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "stlist = [\n",
    "    \"INFY,2017-05-01,2000,2164550\",\n",
    "    \"INFY,2017-5-02,1954,2174352\",\n",
    "    \"INFY,2017-06-03,2341,2934231\",\n",
    "    \"INFY,2017-06-04,1814,1904557\",\n",
    "    \"SBIN,2017-05-01,200061,3164550\",\n",
    "    \"SBIN,2017-5-02,211954,3174352\",\n",
    "    \"SBIN,2017-06-03,222341,3434234\",\n",
    "    \"SBIN,2017-06-04,301814,4590455\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- symbol: string (nullable = true)\n",
      " |-- trdate: string (nullable = true)\n",
      " |-- qty: integer (nullable = true)\n",
      " |-- vlu: double (nullable = true)\n",
      "\n",
      "None\n",
      "+------+----------+------+---------+\n",
      "|symbol|    trdate|   qty|      vlu|\n",
      "+------+----------+------+---------+\n",
      "|  INFY|2017-05-01|  2000|2164550.0|\n",
      "|  INFY| 2017-5-02|  1954|2174352.0|\n",
      "|  INFY|2017-06-03|  2341|2934231.0|\n",
      "|  INFY|2017-06-04|  1814|1904557.0|\n",
      "|  SBIN|2017-05-01|200061|3164550.0|\n",
      "|  SBIN| 2017-5-02|211954|3174352.0|\n",
      "|  SBIN|2017-06-03|222341|3434234.0|\n",
      "|  SBIN|2017-06-04|301814|4590455.0|\n",
      "+------+----------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "stock_schema = StructType(\n",
    "[StructField('symbol', StringType()), StructField('trdate', StringType()), \n",
    "StructField('qty', IntegerType()), StructField('vlu', DoubleType())])\n",
    "stock_rdd = sc.parallelize(stlist).map(lambda x: x.split(',')).map(\n",
    "    blambda x: (x[0], x[1], int(x[2]), float(x[3])))\n",
    "stdf = spark.createDataFrame(stock_rdd, stock_schema)\n",
    "print(stdf.printSchema())\n",
    "stdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+------+---------+\n",
      "|symbol|  yr|mnth|   qty|      vlu|\n",
      "+------+----+----+------+---------+\n",
      "|  INFY|2017|   5|  2000|2164550.0|\n",
      "|  INFY|2017|   5|  1954|2174352.0|\n",
      "|  INFY|2017|   6|  2341|2934231.0|\n",
      "|  INFY|2017|   6|  1814|1904557.0|\n",
      "|  SBIN|2017|   5|200061|3164550.0|\n",
      "|  SBIN|2017|   5|211954|3174352.0|\n",
      "|  SBIN|2017|   6|222341|3434234.0|\n",
      "|  SBIN|2017|   6|301814|4590455.0|\n",
      "+------+----+----+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stdf_wym = stdf.select(\"symbol\", year(to_date(\"trdate\")).alias(\"yr\"),\n",
    "month(to_date(\"trdate\")).alias(\"mnth\"), \"qty\", \"vlu\")\n",
    "stdf_wym.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manipulating spark data frames using the underlying rdd\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('INFY', '2017-05-01', 2000, 2164550.0),\n",
       " ('INFY', '2017-5-02', 1954, 2174352.0),\n",
       " ('INFY', '2017-06-03', 2341, 2934231.0),\n",
       " ('INFY', '2017-06-04', 1814, 1904557.0),\n",
       " ('SBIN', '2017-05-01', 200061, 3164550.0),\n",
       " ('SBIN', '2017-5-02', 211954, 3174352.0),\n",
       " ('SBIN', '2017-06-03', 222341, 3434234.0),\n",
       " ('SBIN', '2017-06-04', 301814, 4590455.0)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Manipulating spark data frames using the underlying rdd')\n",
    "stdf.rdd.map(lambda x: (x[0], x[1], x[2], x[3])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cube will provide every cobmination of the fields used to create the cube\n",
      "+------+----+----+------+-----------+\n",
      "|symbol|  yr|mnth|   qty|        vlu|\n",
      "+------+----+----+------+-----------+\n",
      "|  SBIN|null|null|936170|1.4363591E7|\n",
      "|  SBIN|2017|   6|524155|  8024689.0|\n",
      "|  null|2017|   5|415969|1.0677804E7|\n",
      "|  SBIN|null|   5|412015|  6338902.0|\n",
      "|  null|null|   5|415969|1.0677804E7|\n",
      "|  SBIN|2017|null|936170|1.4363591E7|\n",
      "|  SBIN|2017|   5|412015|  6338902.0|\n",
      "|  null|2017|   6|528310|1.2863477E7|\n",
      "|  INFY|null|null|  8109|  9177690.0|\n",
      "|  SBIN|null|   6|524155|  8024689.0|\n",
      "|  INFY|null|   6|  4155|  4838788.0|\n",
      "|  null|null|null|944279|2.3541281E7|\n",
      "|  INFY|null|   5|  3954|  4338902.0|\n",
      "|  INFY|2017|   5|  3954|  4338902.0|\n",
      "|  null|null|   6|528310|1.2863477E7|\n",
      "|  INFY|2017|null|  8109|  9177690.0|\n",
      "|  null|2017|null|944279|2.3541281E7|\n",
      "|  INFY|2017|   6|  4155|  4838788.0|\n",
      "+------+----+----+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"cube will provide every cobmination of the fields used to create the cube\")\n",
    "stdf_wym.cube(\"symbol\", \"yr\", \"mnth\").agg(sum(\"qty\").alias(\"qty\"), sum(\"vlu\").alias(\"vlu\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "rollup will rollup aggregates beginning from the first field in the rollup columns\n",
      "+------+----+----+------+-----------+\n",
      "|symbol|  yr|mnth|   qty|        vlu|\n",
      "+------+----+----+------+-----------+\n",
      "|  SBIN|null|null|936170|1.4363591E7|\n",
      "|  SBIN|2017|   6|524155|  8024689.0|\n",
      "|  SBIN|2017|null|936170|1.4363591E7|\n",
      "|  SBIN|2017|   5|412015|  6338902.0|\n",
      "|  INFY|null|null|  8109|  9177690.0|\n",
      "|  null|null|null|944279|2.3541281E7|\n",
      "|  INFY|2017|   5|  3954|  4338902.0|\n",
      "|  INFY|2017|null|  8109|  9177690.0|\n",
      "|  INFY|2017|   6|  4155|  4838788.0|\n",
      "+------+----+----+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nrollup will rollup aggregates beginning from the first field in the rollup columns\")\n",
    "stdf_wym.rollup(\"symbol\", \"yr\", \"mnth\").agg(sum(\"qty\").alias(\"qty\"), sum(\"vlu\").alias(\"vlu\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdf_wym.registerTempTable(\"stmdtbl\")\n",
    "stdf_wym.write.mode('overwrite').parquet(\"hdfs://localhost:8020/user/cloudera/stdf_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using sql to carry out multi dimensional aggregations\n",
      "+------+----+----+------+-----------+\n",
      "|symbol|  yr|mnth|   qty|        vlu|\n",
      "+------+----+----+------+-----------+\n",
      "|  SBIN|null|null|936170|1.4363591E7|\n",
      "|  SBIN|2017|   6|524155|  8024689.0|\n",
      "|  null|2017|   5|415969|1.0677804E7|\n",
      "|  SBIN|null|   5|412015|  6338902.0|\n",
      "|  null|null|   5|415969|1.0677804E7|\n",
      "|  SBIN|2017|null|936170|1.4363591E7|\n",
      "|  SBIN|2017|   5|412015|  6338902.0|\n",
      "|  null|2017|   6|528310|1.2863477E7|\n",
      "|  INFY|null|null|  8109|  9177690.0|\n",
      "|  SBIN|null|   6|524155|  8024689.0|\n",
      "|  INFY|null|   6|  4155|  4838788.0|\n",
      "|  null|null|null|944279|2.3541281E7|\n",
      "|  INFY|null|   5|  3954|  4338902.0|\n",
      "|  INFY|2017|   5|  3954|  4338902.0|\n",
      "|  null|null|   6|528310|1.2863477E7|\n",
      "|  INFY|2017|null|  8109|  9177690.0|\n",
      "|  null|2017|null|944279|2.3541281E7|\n",
      "|  INFY|2017|   6|  4155|  4838788.0|\n",
      "+------+----+----+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"create external table if not exists\n",
    "            stmdtbl(symbol string,yr int, mnth int, qty int, vlu double )\n",
    "            stored as parquet\n",
    "            location 'hdfs://localhost:8020/user/cloudera/stdf_parquet'\"\"\")\n",
    "\n",
    "print(\"Using sql to carry out multi dimensional aggregations\")\n",
    "spark.sql(\"\"\"select symbol,yr,mnth,sum(qty) as qty, sum(vlu) as vlu\n",
    "          from stmdtbl\n",
    "          group by symbol, yr, mnth\n",
    "          with cube\"\"\").show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

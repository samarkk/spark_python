{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "spark_home = '/usr/lib/spark'\n",
    "os.environ['SPARK_HOME']= spark_home\n",
    "os.environ['PYLIB']=os.environ['SPARK_HOME']+'/python/lib'\n",
    "sys.path.insert(0,os.environ['PYLIB']+'/py4j-0.10.7-src.zip')\n",
    "sys.path.insert(1,os.environ['PYLIB']+'/pyspark.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('ForestCoverTypeClassifier') \\\n",
    ".config('spark.warehouse.dir','/apps/hive/warehouse') \\\n",
    ".config('spark.driver.memory', '4G') \\\n",
    ".config('spark.sql.shuffle.partitions', 4) \\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Having set the driver and driver options we should have spark representing spark session \n",
    "# available straight away\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   KDD cup was like kaggle before there was kaggle\n",
    "#   1999 topic was network intrusion and data is still available\n",
    "#   data available at http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n",
    "#   708 mb in size with 4.89 million  csv rows - each spanning 42 values\n",
    "#   there is also the ten percent sample data set with 490k rows\n",
    "#   data column names are available from kdd.names file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Covtype dataset publicly available dataset provides information on\n",
    "# types of forest-covering parcels of land in Colorado, USA\n",
    "fileloc = \"D:/ufdata/kddcup.testdata_10_percent\"\n",
    "data = spark.read \\\n",
    ".option(\"inferSchema\", \"true\") \\\n",
    ".option(\"header\", \"false\") \\\n",
    ".csv(\"D:/ufdata/kddcup.data_10_percent\") \\\n",
    ".toDF(\n",
    "      \"duration\", \"protocol_type\", \"service\", \"flag\",\n",
    "      \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\",\n",
    "      \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\",\n",
    "      \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\",\n",
    "      \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
    "      \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\",\n",
    "      \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\",\n",
    "      \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\",\n",
    "      \"dst_host_count\", \"dst_host_srv_count\",\n",
    "      \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\",\n",
    "      \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\",\n",
    "      \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n",
    "      \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\",\n",
    "      \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The labels and their count - 23 distinct labels with most frequent being smurf. and neptune.\n",
      "+----------------+------+\n",
      "|           label| count|\n",
      "+----------------+------+\n",
      "|          smurf.|280790|\n",
      "|        neptune.|107201|\n",
      "|         normal.| 97278|\n",
      "|           back.|  2203|\n",
      "|          satan.|  1589|\n",
      "|        ipsweep.|  1247|\n",
      "|      portsweep.|  1040|\n",
      "|    warezclient.|  1020|\n",
      "|       teardrop.|   979|\n",
      "|            pod.|   264|\n",
      "|           nmap.|   231|\n",
      "|   guess_passwd.|    53|\n",
      "|buffer_overflow.|    30|\n",
      "|           land.|    21|\n",
      "|    warezmaster.|    20|\n",
      "|           imap.|    12|\n",
      "|        rootkit.|    10|\n",
      "|     loadmodule.|     9|\n",
      "|      ftp_write.|     8|\n",
      "|       multihop.|     7|\n",
      "|            phf.|     4|\n",
      "|           perl.|     3|\n",
      "|            spy.|     2|\n",
      "+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  take a look at the labels\n",
    "#  in unsupervised learning we do not use the label - however it is here useful to\n",
    "#  find the labels present in the data - normal and various type of classified network attacks\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "print(\"\\nThe labels and their count - 23 distinct labels with most frequent being smurf. and neptune.\")\n",
    "data.select(\"label\").groupBy(\"label\").count().orderBy(desc('count')).show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the non numeric columns\n",
    "numericOnly = data.drop(\"protocol_type\", \"service\", \"flag\").cache()\n",
    "\n",
    "# assemble them into a vector leaving the label out\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler() \\\n",
    ".setInputCols(list(filter(lambda x: x != \"label\", numericOnly.columns))) \\\n",
    ".setOutputCol(\"featureVector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint, seed\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans() \\\n",
    ".setSeed(100) \\\n",
    ".setPredictionCol(\"cluster\") \\\n",
    ".setFeaturesCol(\"featureVector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "pipeline = Pipeline().setStages([assembler, kmeans])\n",
    "pipelineModel = pipeline.fit(numericOnly)\n",
    "kmeansModel = pipelineModel.stages[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with no parameter setting, kmeans creates two clusters\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([4.79793956e+01, 1.62207883e+03, 8.68534183e+02, 4.45326100e-05,\n",
       "        6.43293794e-03, 1.41694668e-05, 3.45168212e-02, 1.51815716e-04,\n",
       "        1.48247035e-01, 1.02121372e-02, 1.11331525e-04, 3.64357718e-05,\n",
       "        1.13517671e-02, 1.08295211e-03, 1.09307315e-04, 1.00805635e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.38658354e-03, 3.32286248e+02,\n",
       "        2.92907143e+02, 1.76685418e-01, 1.76607809e-01, 5.74330999e-02,\n",
       "        5.77183920e-02, 7.91548844e-01, 2.09816404e-02, 2.89968625e-02,\n",
       "        2.32470732e+02, 1.88666046e+02, 7.53781203e-01, 3.09056111e-02,\n",
       "        6.01935529e-01, 6.68351484e-03, 1.76753957e-01, 1.76441622e-01,\n",
       "        5.81176268e-02, 5.74111170e-02]),\n",
       " array([2.0000000e+00, 6.9337564e+08, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 1.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 5.7000000e+01,\n",
       "        3.0000000e+00, 7.9000000e-01, 6.7000000e-01, 2.1000000e-01,\n",
       "        3.3000000e-01, 5.0000000e-02, 3.9000000e-01, 0.0000000e+00,\n",
       "        2.5500000e+02, 3.0000000e+00, 1.0000000e-02, 9.0000000e-02,\n",
       "        2.2000000e-01, 0.0000000e+00, 1.8000000e-01, 6.7000000e-01,\n",
       "        5.0000000e-02, 3.3000000e-01])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"with no parameter setting, kmeans creates two clusters\\n\")\n",
    "kmeansModel.clusterCenters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Taking a look at the clusters generated and the labels assigned\n",
      "+-------+----------------+------+\n",
      "|cluster|label           |count |\n",
      "+-------+----------------+------+\n",
      "|0      |smurf.          |280790|\n",
      "|0      |neptune.        |107201|\n",
      "|0      |normal.         |97278 |\n",
      "|0      |back.           |2203  |\n",
      "|0      |satan.          |1589  |\n",
      "|0      |ipsweep.        |1247  |\n",
      "|0      |portsweep.      |1039  |\n",
      "|0      |warezclient.    |1020  |\n",
      "|0      |teardrop.       |979   |\n",
      "|0      |pod.            |264   |\n",
      "|0      |nmap.           |231   |\n",
      "|0      |guess_passwd.   |53    |\n",
      "|0      |buffer_overflow.|30    |\n",
      "|0      |land.           |21    |\n",
      "|0      |warezmaster.    |20    |\n",
      "|0      |imap.           |12    |\n",
      "|0      |rootkit.        |10    |\n",
      "|0      |loadmodule.     |9     |\n",
      "|0      |ftp_write.      |8     |\n",
      "|0      |multihop.       |7     |\n",
      "|0      |phf.            |4     |\n",
      "|0      |perl.           |3     |\n",
      "|0      |spy.            |2     |\n",
      "|1      |portsweep.      |1     |\n",
      "+-------+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use the model transformer to add cluster information to the dataset we gave to the\n",
    "# estimator to generate the model\n",
    "\n",
    "withCluster = pipelineModel.transform(numericOnly)\n",
    "\n",
    "print(\"\\nTaking a look at the clusters generated and the labels assigned\")\n",
    "withCluster.select(\"cluster\", \"label\"). \\\n",
    "groupBy(\"cluster\", \"label\").count(). \\\n",
    "orderBy(\"cluster\", desc('count')). \\\n",
    "show(50, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us try with different values of k the best possible score that we wil get\n",
    "def clusteringScore0(data, k):\n",
    "    \n",
    "    assembler = VectorAssembler().\\\n",
    "    setInputCols(list(filter(lambda x: x != \"label\", data.columns))). \\\n",
    "    setOutputCol(\"featureVector\")\n",
    "\n",
    "    kmeans = KMeans(). \\\n",
    "    setSeed(100). \\\n",
    "    setK(k). \\\n",
    "    setPredictionCol(\"cluster\"). \\\n",
    "    setFeaturesCol(\"featureVector\")\n",
    "\n",
    "    pipeline = Pipeline().setStages([assembler, kmeans])\n",
    "\n",
    "    kmeansModel = pipeline.fit(data).stages[1]\n",
    "    return kmeansModel.computeCost(assembler.transform(data)) / data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Take 0 - the cost with varied number of clusters\n",
      "[(20, 70090529.54393287), (40, 69889095.66546552), (60, 34130705.51573093), (80, 6281197.831342092), (100, 5249377.999851539), (120, 3552517.5381736914), (140, 2580194.8646263178)]\n"
     ]
    }
   ],
   "source": [
    "# take 0\n",
    "print(\"\\nTake 0 - the cost with varied number of clusters\")\n",
    "  \n",
    "# We see that the cost is lowest for 80\n",
    "# if we were to take as many clusters as the number of points, the cost would be 0\n",
    "\n",
    "print(list(map(lambda k: (k, clusteringScore0(numericOnly, k)), range(20, 160, 20))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusteringScore1(data, k):\n",
    "    \n",
    "    assembler = VectorAssembler().\\\n",
    "    setInputCols(list(filter(lambda x: x != \"label\", data.columns))). \\\n",
    "    setOutputCol(\"featureVector\")\n",
    "\n",
    "    kmeans = KMeans(). \\\n",
    "    setSeed(100). \\\n",
    "    setK(k). \\\n",
    "    setPredictionCol(\"cluster\"). \\\n",
    "    setFeaturesCol(\"featureVector\"). \\\n",
    "    setMaxIter(40).setTol(1.0e-5)\n",
    "\n",
    "    pipeline = Pipeline().setStages([assembler, kmeans])\n",
    "\n",
    "    kmeansModel = pipeline.fit(data).stages[1]\n",
    "    return kmeansModel.computeCost(assembler.transform(data)) / data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Take 1 - the cost where we have explicitly set the number of iterations and tolerance\n",
      "[(20, 70090529.54393286), (40, 69889095.66546552), (60, 34130705.51573093), (80, 6281197.831342092), (100, 5249377.999851539), (120, 3552516.214068078), (140, 2580190.5810112343)]\n"
     ]
    }
   ],
   "source": [
    "# take 1\n",
    "print(\"\\nTake 1 - the cost where we have explicitly set the number of iterations and tolerance\")\n",
    "\n",
    "# when we run more iterations and set the tolerance level, we see\n",
    "# that the clustering cost is lowest for 60\n",
    "# higher number of clusters should have a lower cost\n",
    "# the problem may be suboptimal clustering due to ineffective starting points or not low\n",
    "# enough distance discriminator or both\n",
    "print(list(map(lambda k: (k, clusteringScore1(numericOnly, k)), range(20, 160, 20))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we are going to normalize the data and check\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "def clusteringScore2(data, k):\n",
    "    \n",
    "    assembler = VectorAssembler().\\\n",
    "    setInputCols(list(filter(lambda x: x != \"label\", data.columns))). \\\n",
    "    setOutputCol(\"featureVector\")\n",
    "    \n",
    "    scaler = StandardScaler(). \\\n",
    "    setInputCol(\"featureVector\"). \\\n",
    "    setOutputCol(\"scaledFeatureVector\"). \\\n",
    "    setWithStd(True). \\\n",
    "    setWithMean(False)\n",
    "    \n",
    "    kmeans = KMeans(). \\\n",
    "    setSeed(100). \\\n",
    "    setK(k). \\\n",
    "    setPredictionCol(\"cluster\"). \\\n",
    "    setFeaturesCol(\"scaledFeatureVector\"). \\\n",
    "    setMaxIter(40).setTol(1.0e-5)\n",
    "\n",
    "    pipeline = Pipeline().setStages([assembler, scaler,  kmeans])\n",
    "    pipelineModel = pipeline.fit(data)\n",
    "    \n",
    "    kmeansModel = pipeline.fit(data).stages[2]\n",
    "    \n",
    "    return kmeansModel.computeCost(pipelineModel.transform(data)) / data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Take 2 - Consider the scores after we have scaled, normalized the data\n",
      "[(20, 7.662614419004401), (40, 1.8592876574979695), (60, 1.0796203270258335), (80, 0.7950829979073035), (100, 0.5719722421002665), (120, 0.5222554892359224), (140, 0.4091315697738147)]\n"
     ]
    }
   ],
   "source": [
    "# take 2\n",
    "print(\"\\nTake 2 - Consider the scores after we have scaled, normalized the data\")\n",
    "print(list(map(lambda k: (k, clusteringScore2(numericOnly, k)), range(20, 160, 20))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will include the non numeric columns and check\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "def oneHotPipeline(inputCol): \n",
    "    indexer =  StringIndexer(). \\\n",
    "    setInputCol(inputCol).\\\n",
    "    setOutputCol(inputCol + \"_indexed\")\n",
    "\n",
    "    encoder =  OneHotEncoder().\\\n",
    "    setInputCol(inputCol + \"_indexed\").\\\n",
    "    setOutputCol(inputCol + \"_vec\")\n",
    "\n",
    "    pipeline = Pipeline().setStages([indexer, encoder])\n",
    "    return(pipeline, inputCol + \"_vec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusteringScore3(data, k):\n",
    "    protoTypeEncoder, protoTypeVecCol = oneHotPipeline(\"protocol_type\")\n",
    "    serviceEncoder, serviceVecCol = oneHotPipeline(\"service\")\n",
    "    flagEncoder, flagVecCol = oneHotPipeline(\"flag\")\n",
    "    \n",
    "    assembleCols = data.columns\n",
    "#     print(assembleCols)\n",
    "    [assembleCols.remove(col) for col in ['label', 'protocol_type', 'service', 'flag' ]]\n",
    "    [assembleCols.append(col) for col in [protoTypeVecCol, serviceVecCol, flagVecCol]]\n",
    "#     print(assembleCols)\n",
    "    \n",
    "    assembler = VectorAssembler().\\\n",
    "    setInputCols(assembleCols). \\\n",
    "    setOutputCol(\"featureVector\")\n",
    "    \n",
    "    scaler = StandardScaler(). \\\n",
    "    setInputCol(\"featureVector\"). \\\n",
    "    setOutputCol(\"scaledFeatureVector\"). \\\n",
    "    setWithStd(True). \\\n",
    "    setWithMean(False)\n",
    "    \n",
    "    kmeans = KMeans(). \\\n",
    "    setSeed(100). \\\n",
    "    setK(k). \\\n",
    "    setPredictionCol(\"cluster\"). \\\n",
    "    setFeaturesCol(\"scaledFeatureVector\"). \\\n",
    "    setMaxIter(40).setTol(1.0e-5)\n",
    "\n",
    "    pipeline = Pipeline().setStages([protoTypeEncoder, serviceEncoder, flagEncoder, assembler, scaler,  kmeans])\n",
    "    pipelineModel = pipeline.fit(data)\n",
    "    \n",
    "    kmeansModel = pipeline.fit(data).stages[-1]\n",
    "    \n",
    "    return kmeansModel.computeCost(pipelineModel.transform(data)) / data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1, 2, 3][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Take 3 - Clustering cost scores after one hot encoding string columns\n",
      "[(20, 77.75052364318653), (40, 52.649900811145976), (60, 32.828472137573954), (80, 21.375243596683305), (100, 7.1470637725616175), (120, 3.101839490018694), (140, 2.4624693714793167)]\n"
     ]
    }
   ],
   "source": [
    "# take 3\n",
    "print(\"\\nTake 3 - Clustering cost scores after one hot encoding string columns\")\n",
    "#  we have all columns included and now the elbow seems to be around 180/210\n",
    "print(list(map(lambda k: (k, clusteringScore3(data, k)), range(20, 160, 20))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use entropy to see the quality of individual clusters\n",
    "# we will use the labels to compute entropy\n",
    "# more the dominance of a single label, lower will be the entropy\n",
    "# more distributed the presence of different labels, higher will be the entropy \n",
    "# thus more a cluster exhibits skewness towards a single entity, lower will be the entropy and vice versa\n",
    "import math\n",
    "import builtins\n",
    "def entropy(counts):\n",
    "    from builtins import sum as pysum\n",
    "    from math import log as pylog\n",
    "    values = list(filter(lambda x: x > 0, counts))\n",
    "    n = pysum(values)\n",
    "    return pysum(map(lambda v: - v / n * pylog(v / n), values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.277034259466139\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.6578814551411559"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(entropy([100, 200, 300, 100]))\n",
    "-100/700 * math.log(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitPipeline4(data, k):\n",
    "    protoTypeEncoder, protoTypeVecCol = oneHotPipeline(\"protocol_type\")\n",
    "    serviceEncoder, serviceVecCol = oneHotPipeline(\"service\")\n",
    "    flagEncoder, flagVecCol = oneHotPipeline(\"flag\")\n",
    "    \n",
    "    assembleCols = data.columns\n",
    "#     print(assembleCols)\n",
    "    [assembleCols.remove(col) for col in ['label', 'protocol_type', 'service', 'flag' ]]\n",
    "    [assembleCols.append(col) for col in [protoTypeVecCol, serviceVecCol, flagVecCol]]\n",
    "#     print(assembleCols)\n",
    "    \n",
    "    assembler = VectorAssembler().\\\n",
    "    setInputCols(assembleCols). \\\n",
    "    setOutputCol(\"featureVector\")\n",
    "    \n",
    "    scaler = StandardScaler(). \\\n",
    "    setInputCol(\"featureVector\"). \\\n",
    "    setOutputCol(\"scaledFeatureVector\"). \\\n",
    "    setWithStd(True). \\\n",
    "    setWithMean(False)\n",
    "    \n",
    "    kmeans = KMeans(). \\\n",
    "    setSeed(100). \\\n",
    "    setK(k). \\\n",
    "    setPredictionCol(\"cluster\"). \\\n",
    "    setFeaturesCol(\"scaledFeatureVector\"). \\\n",
    "    setMaxIter(40).setTol(1.0e-5)\n",
    "\n",
    "    pipeline = Pipeline().setStages([protoTypeEncoder, serviceEncoder, flagEncoder, assembler, scaler,  kmeans])\n",
    "    pipelineModel = pipeline.fit(data)\n",
    "    \n",
    "    return pipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = fitPipeline4(data, 20).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|cluster|label  |\n",
      "+-------+-------+\n",
      "|1      |normal.|\n",
      "|1      |normal.|\n",
      "|1      |normal.|\n",
      "|1      |normal.|\n",
      "|1      |normal.|\n",
      "|1      |normal.|\n",
      "|1      |normal.|\n",
      "|1      |normal.|\n",
      "|1      |normal.|\n",
      "|1      |normal.|\n",
      "|1      |normal.|\n",
      "|1      |normal.|\n",
      "|1      |normal.|\n",
      "|1      |normal.|\n",
      "|1      |normal.|\n",
      "|1      |normal.|\n",
      "|1      |normal.|\n",
      "|1      |normal.|\n",
      "|1      |normal.|\n",
      "|1      |normal.|\n",
      "+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.select('cluster', 'label').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494021"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "259790.46663264162"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.select('cluster', 'label').rdd.map(lambda x: (x[0], x[1])) \\\n",
    ".groupByKey().map(lambda x: (x[0], [list(x[1]).count(v) for v in set(x[1])], len(list(x[1])))) \\\n",
    ".map(lambda x: (x[0], entropy(x[1]), x[2])) \\\n",
    ".map(lambda x: (x[0], (x[1] * x[2]))) \\\n",
    ".map(lambda x: x[1]) \\\n",
    ".sum()\n",
    "# .map(lambda x: (x[0], len(x[1][1]))).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusteringScore4(data, k):\n",
    "    cluster_df = fitPipeline4(data, k).transform(data)\n",
    "    return cluster_df.select('cluster', 'label').rdd.map(lambda x: (x[0], x[1])) \\\n",
    "    .groupByKey().map(lambda x: (x[0], [list(x[1]).count(v) for v in set(x[1])], len(list(x[1])))) \\\n",
    "    .map(lambda x: (x[0], entropy(x[1]), x[2])) \\\n",
    "    .map(lambda x: x[1] * x[2]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "259790.46663264162"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusteringScore4(data, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Take 4 - Now going for clustering score 4 - estimating the entropy - best score to be found for k = 100\n",
      "9379.615168599032\n"
     ]
    }
   ],
   "source": [
    "# take 4\n",
    "# the best clustering is to be found around 180 - and every iteration takes time\n",
    "# so on a local cluster use the known\n",
    "print(\"\\nTake 4 - Now going for clustering score 4 - estimating the entropy - best score to be found for k = 100\")\n",
    "print(clusteringScore4(data, 180))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|cluster|scaledFeatureVector                                                                                                                                                                                                                                                                                        |\n",
      "+-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|177    |(115,[1,2,8,19,20,25,28,29,30,32,39,42,105],[1.8315794844034206E-4,0.1649515675987804,2.814168444874875,0.03753270996838479,0.032477705818326735,2.5760614800997814,0.1390060564670224,0.08487328273976684,2.434387313317314,0.22854329046843316,2.055363006399789,2.9721197203276812,2.362126925892916])  |\n",
      "|177    |(115,[1,2,8,19,20,25,28,29,30,32,39,42,105],[2.418494457306174E-4,0.014709442541836199,2.814168444874875,0.03753270996838479,0.032477705818326735,2.5760614800997814,0.2934572303192695,0.17917693022839665,2.434387313317314,0.1038833138492878,2.055363006399789,2.9721197203276812,2.362126925892916])  |\n",
      "|177    |(115,[1,2,8,19,20,25,28,29,30,32,39,42,105],[2.3780175626232254E-4,0.040466100161388886,2.814168444874875,0.03753270996838479,0.032477705818326735,2.5760614800997814,0.4479084041715166,0.2734805777170265,2.434387313317314,0.062329988309572676,2.055363006399789,2.9721197203276812,2.362126925892916])|\n",
      "|177    |(115,[1,2,8,19,20,25,28,29,30,32,39,42,105],[2.2161099838914313E-4,0.040466100161388886,2.814168444874875,0.028149532476288597,0.02435827936374505,2.5760614800997814,0.6023595780237637,0.3677842252056563,2.434387313317314,0.062329988309572676,2.055363006399789,2.9721197203276812,2.362126925892916])|\n",
      "|177    |(115,[1,2,8,19,20,25,28,29,30,32,39,42,105],[2.1958715365499572E-4,0.06150120832306822,2.814168444874875,0.028149532476288597,0.02435827936374505,2.5760614800997814,0.7568107518760109,0.46208787269428614,2.434387313317314,0.04155332553971512,2.055363006399789,2.9721197203276812,2.362126925892916]) |\n",
      "+-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "threshold is  704.0392967608535\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vector, Vectors\n",
    "pipelineModel = fitPipeline4(data, 180)\n",
    "\n",
    "kmeansModel = pipelineModel.stages[-1]\n",
    "centroids = kmeansModel.clusterCenters()\n",
    "# print(centroids[0])\n",
    "# print(len(centroids[0]))\n",
    "\n",
    "clustered = pipelineModel.transform(data)\n",
    "clustered.select('cluster', 'scaledFeatureVector').show(5, False)\n",
    "\n",
    "threshold_boundary = 100\n",
    "\n",
    "threshold = clustered. \\\n",
    "select(\"cluster\", \"scaledFeatureVector\").rdd. \\\n",
    "map(lambda x: Vectors.squared_distance(Vectors.dense(centroids[x[0]]), x[1])) \\\n",
    ".sortBy(lambda x: -x).take(threshold_boundary)[threshold_boundary - 1]\n",
    "\n",
    "print('threshold is ' , threshold)\n",
    "\n",
    "\n",
    "# map { case (cluster, vec) => Vectors.sqdist(centroids(cluster), vec) }.\n",
    "#       orderBy($\"value\".desc).take(100).last\n",
    "\n",
    "#     val originalCols = data.columns\n",
    "#     val anomalies = clustered.filter { row =>\n",
    "#       val cluster = row.getAs[Int](\"cluster\")\n",
    "#       val vec = row.getAs[Vector](\"scaledFeatureVector\")\n",
    "#       Vectors.sqdist(centroids(cluster), vec) >= threshold\n",
    "#     }.select(originalCols.head, originalCols.tail: _*)\n",
    "\n",
    "#     println(\"\\nPrinting the first ten anomalous looking entries\")\n",
    "#     anomalies.take(10).foreach(println)\n",
    "\n",
    "#     println(\"Total number of anomalous points found: \" + anomalies.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clustered.withColumn('id', monotonically_increasing_id()).select('id').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_with_id = clustered.withColumn('id', monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11882, 2171.968313051754),\n",
       " (15699, 1226.0213560587104),\n",
       " (16064, 857.4055273940494),\n",
       " (21931, 2092.4015736714387),\n",
       " (22750, 3337.0915028250834)]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomalies_rdd = clustered_with_id.select(\"id\", \"cluster\", \"scaledFeatureVector\").rdd. \\\n",
    "map(lambda x: (x[0], Vectors.squared_distance(Vectors.dense(centroids[x[1]]), x[2]))) \\\n",
    ".filter(lambda x: x[1] >= threshold)\n",
    "anomalies_rdd.cache()\n",
    "anomalies_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total numboer of anomailes  100\n"
     ]
    }
   ],
   "source": [
    "print('total numboer of anomailes ', anomalies_rdd.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "not supported type: <class 'numpy.float64'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mD:\\spark231hdp27\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\u001b[0m in \u001b[0;36m_infer_type\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   1067\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1068\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_infer_schema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1069\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\spark231hdp27\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\u001b[0m in \u001b[0;36m_infer_schema\u001b[1;34m(row, names)\u001b[0m\n\u001b[0;32m   1093\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1094\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Can not infer schema for type: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1095\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Can not infer schema for type: <class 'numpy.float64'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-164-d06756639477>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0manomalies_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manomalies_rdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'distance'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0manomalies_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\spark231hdp27\\python\\lib\\pyspark.zip\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mtoDF\u001b[1;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Alice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \"\"\"\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\spark231hdp27\\python\\lib\\pyspark.zip\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 689\u001b[1;33m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    690\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\spark231hdp27\\python\\lib\\pyspark.zip\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[1;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[0;32m    382\u001b[0m         \"\"\"\n\u001b[0;32m    383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m             \u001b[0mstruct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\spark231hdp27\\python\\lib\\pyspark.zip\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[1;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msamplingRatio\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m             \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_infer_schema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\spark231hdp27\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\u001b[0m in \u001b[0;36m_infer_schema\u001b[1;34m(row, names)\u001b[0m\n\u001b[0;32m   1094\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Can not infer schema for type: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1095\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1096\u001b[1;33m     \u001b[0mfields\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mStructField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_infer_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1097\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mStructType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\spark231hdp27\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1094\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Can not infer schema for type: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1095\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1096\u001b[1;33m     \u001b[0mfields\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mStructField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_infer_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1097\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mStructType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\spark231hdp27\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\u001b[0m in \u001b[0;36m_infer_type\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   1068\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_infer_schema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1069\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1070\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"not supported type: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1071\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1072\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: not supported type: <class 'numpy.float64'>"
     ]
    }
   ],
   "source": [
    "anomalies_df = anomalies_rdd.toDF(['id', 'distance'])\n",
    "anomalies_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|   id|          distance|\n",
      "+-----+------------------+\n",
      "|11882| 2171.968313051754|\n",
      "|15699|1226.0213560587104|\n",
      "|16064| 857.4055273940494|\n",
      "|21931|2092.4015736714387|\n",
      "|22750|3337.0915028250834|\n",
      "|22772|3357.6963259464815|\n",
      "|22785| 1052.185251221034|\n",
      "|22786|1052.2850282222416|\n",
      "|22789| 1052.285234229538|\n",
      "|22790|1052.2810046753782|\n",
      "|23235|  2195.18678162918|\n",
      "|25708| 827.1860873820201|\n",
      "|26541| 862.1152571582687|\n",
      "|26616| 730.0491846706202|\n",
      "|26637| 704.9558735560414|\n",
      "|26638| 709.8037446447116|\n",
      "|26671| 1542.306110702227|\n",
      "|26673| 720.4301114151302|\n",
      "|26771|1526.7587156086972|\n",
      "|31698| 734.3909176287823|\n",
      "+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "anomalies_df = anomalies_rdd.map(lambda x: (x[0], float(x[1]))).toDF(['id', 'distance'])\n",
    "anomalies_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------+----+---------+---------+----+--------------+------+---+-----------------+---------+---------------+----------+------------+--------+------------------+----------+----------------+-----------------+-------------+--------------+-----+---------+-----------+---------------+-----------+---------------+-------------+-------------+------------------+--------------+------------------+----------------------+----------------------+---------------------------+---------------------------+--------------------+------------------------+--------------------+------------------------+-------+\n",
      "|duration|protocol_type|service|flag|src_bytes|dst_bytes|land|wrong_fragment|urgent|hot|num_failed_logins|logged_in|num_compromised|root_shell|su_attempted|num_root|num_file_creations|num_shells|num_access_files|num_outbound_cmds|is_host_login|is_guest_login|count|srv_count|serror_rate|srv_serror_rate|rerror_rate|srv_rerror_rate|same_srv_rate|diff_srv_rate|srv_diff_host_rate|dst_host_count|dst_host_srv_count|dst_host_same_srv_rate|dst_host_diff_srv_rate|dst_host_same_src_port_rate|dst_host_srv_diff_host_rate|dst_host_serror_rate|dst_host_srv_serror_rate|dst_host_rerror_rate|dst_host_srv_rerror_rate|  label|\n",
      "+--------+-------------+-------+----+---------+---------+----+--------------+------+---+-----------------+---------+---------------+----------+------------+--------+------------------+----------+----------------+-----------------+-------------+--------------+-----+---------+-----------+---------------+-----------+---------------+-------------+-------------+------------------+--------------+------------------+----------------------+----------------------+---------------------------+---------------------------+--------------------+------------------------+--------------------+------------------------+-------+\n",
      "|       0|          tcp|   http|  SF|      181|     5450|   0|             0|     0|  0|                0|        1|              0|         0|           0|       0|                 0|         0|               0|                0|            0|             0|    8|        8|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|             9|                 9|                   1.0|                   0.0|                       0.11|                        0.0|                 0.0|                     0.0|                 0.0|                     0.0|normal.|\n",
      "+--------+-------------+-------+----+---------+---------+----+--------------+------+---+-----------------+---------+---------------+----------+------------+--------+------------------+----------+----------------+-----------------+-------------+--------------+-----+---------+-----------+---------------+-----------+---------------+-------------+-------------+------------------+--------------+------------------+----------------------+----------------------+---------------------------+---------------------------+--------------------+------------------------+--------------------+------------------------+-------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "origCols = data.columns\n",
    "data.select(origCols).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+--------+----+---------+---------+----+--------------+------+---+-----------------+---------+---------------+----------+------------+--------+------------------+----------+----------------+-----------------+-------------+--------------+-----+---------+-----------+---------------+-----------+---------------+-------------+-------------+------------------+--------------+------------------+----------------------+----------------------+---------------------------+---------------------------+--------------------+------------------------+--------------------+------------------------+-------------+\n",
      "|duration|protocol_type| service|flag|src_bytes|dst_bytes|land|wrong_fragment|urgent|hot|num_failed_logins|logged_in|num_compromised|root_shell|su_attempted|num_root|num_file_creations|num_shells|num_access_files|num_outbound_cmds|is_host_login|is_guest_login|count|srv_count|serror_rate|srv_serror_rate|rerror_rate|srv_rerror_rate|same_srv_rate|diff_srv_rate|srv_diff_host_rate|dst_host_count|dst_host_srv_count|dst_host_same_srv_rate|dst_host_diff_srv_rate|dst_host_same_src_port_rate|dst_host_srv_diff_host_rate|dst_host_serror_rate|dst_host_srv_serror_rate|dst_host_rerror_rate|dst_host_srv_rerror_rate|        label|\n",
      "+--------+-------------+--------+----+---------+---------+----+--------------+------+---+-----------------+---------+---------------+----------+------------+--------+------------------+----------+----------------+-----------------+-------------+--------------+-----+---------+-----------+---------------+-----------+---------------+-------------+-------------+------------------+--------------+------------------+----------------------+----------------------+---------------------------+---------------------------+--------------------+------------------------+--------------------+------------------------+-------------+\n",
      "|       1|          tcp|     ftp|  SF|       60|      189|   0|             0|     0|  0|                1|        0|              0|         0|           0|       0|                 0|         0|               0|                0|            0|             0|    1|        1|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|           151|                47|                  0.31|                  0.03|                       0.01|                        0.0|                 0.0|                     0.0|                 0.0|                     0.0|      normal.|\n",
      "|      23|          tcp|  telnet|  SF|      104|      276|   0|             0|     0|  0|                5|        0|              0|         0|           0|       0|                 0|         0|               0|                0|            0|             0|    1|        1|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|             1|                 2|                   1.0|                   0.0|                        1.0|                        1.0|                 0.0|                     0.0|                 0.0|                     0.0|guess_passwd.|\n",
      "|       0|          tcp|  telnet|  S1|        0|        0|   0|             0|     0|  0|                0|        0|              0|         0|           0|       0|                 0|         0|               0|                0|            0|             0|    1|        1|        1.0|            1.0|        0.0|            0.0|          1.0|          0.0|               0.0|           126|                 1|                  0.01|                  0.05|                       0.01|                        0.0|                0.01|                     1.0|                 0.0|                     0.0|      normal.|\n",
      "|      60|          tcp|  telnet|  S3|      125|      179|   0|             0|     0|  1|                1|        0|              0|         0|           0|       0|                 0|         0|               0|                0|            0|             0|    1|        1|        1.0|            1.0|        0.0|            0.0|          1.0|          0.0|               0.0|             1|                 1|                   1.0|                   0.0|                        1.0|                        0.0|                 1.0|                     1.0|                 0.0|                     0.0|guess_passwd.|\n",
      "|      60|          tcp|  telnet|  S3|      126|      179|   0|             0|     0|  1|                1|        0|              0|         0|           0|       0|                 0|         0|               0|                0|            0|             0|    2|        2|        0.5|            0.5|        0.5|            0.5|          1.0|          0.0|               0.0|            23|                23|                   1.0|                   0.0|                       0.04|                        0.0|                0.09|                    0.09|                0.91|                    0.91|guess_passwd.|\n",
      "|       0|          tcp|  telnet|RSTR|      126|      179|   0|             0|     0|  2|                1|        0|              0|         0|           0|       0|                 0|         0|               0|                0|            0|             0|    1|        1|        0.0|            0.0|        1.0|            1.0|          1.0|          0.0|               0.0|            40|                40|                   1.0|                   0.0|                       0.03|                        0.0|                0.05|                    0.05|                0.95|                    0.95|guess_passwd.|\n",
      "|      10|          tcp|    http|  SF|      194|   954639|   0|             0|     0|  0|                0|        1|              0|         0|           0|       0|                 0|         0|               0|                0|            0|             0|    1|        1|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|           255|               255|                   1.0|                   0.0|                        0.0|                        0.0|                 0.0|                     0.0|                 0.0|                     0.0|      normal.|\n",
      "|       6|          tcp|  gopher|RSTO|        0|       77|   0|             0|     0|  0|                0|        1|              0|         0|           0|       0|                 0|         0|               0|                0|            0|             0|    1|        1|        0.0|            0.0|        1.0|            1.0|          1.0|          0.0|               0.0|            51|                 2|                  0.02|                   1.0|                       0.02|                        1.0|                 0.0|                     0.0|                 0.9|                     0.5|     ipsweep.|\n",
      "|      34|          tcp|  telnet|  SF|      102|     1025|   0|             0|     0|  0|                1|        1|              0|         0|           0|       0|                 0|         0|               0|                0|            0|             0|    1|        1|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|           255|                 7|                  0.03|                  0.73|                        0.0|                        0.0|                0.72|                     0.0|                 0.0|                     0.0|      normal.|\n",
      "|    6155|          tcp|     IRC|RSTO|     1112|     4968|   0|             0|     0|  0|                0|        1|              0|         0|           0|       0|                 0|         0|               0|                0|            0|             0|    1|        1|        0.0|            0.0|        1.0|            1.0|          1.0|          0.0|               0.0|            16|                 8|                   0.5|                  0.12|                       0.06|                        0.0|                 0.0|                     0.0|                0.44|                    0.88|      normal.|\n",
      "|       6|          tcp|    http|  SF|      167|  3916592|   0|             0|     0|  0|                0|        1|              0|         0|           0|       0|                 0|         0|               0|                0|            0|             0|    1|        1|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|           131|               255|                   1.0|                   0.0|                       0.01|                       0.01|                 0.0|                     0.0|                 0.0|                     0.0|      normal.|\n",
      "|       0|          tcp|    http|  S3|      307|     2698|   0|             0|     0|  0|                0|        1|              0|         0|           0|       0|                 0|         0|               1|                0|            0|             0|    4|        4|       0.25|           0.25|        0.0|            0.0|          1.0|          0.0|               0.0|           220|               198|                   0.9|                  0.05|                        0.0|                        0.0|                0.02|                    0.01|                 0.0|                     0.0|      normal.|\n",
      "|   40448|          tcp|csnet_ns|RSTR|        1|        0|   0|             0|     0|  0|                0|        0|              0|         0|           0|       0|                 0|         0|               0|                0|            0|             0|    2|        2|        0.0|            0.0|        1.0|            1.0|          1.0|          0.0|               0.0|           255|                 2|                  0.01|                  0.42|                       0.82|                        0.0|                 0.0|                     0.0|                0.82|                     1.0|   portsweep.|\n",
      "|   30619|          tcp| courier|RSTR|        1|        0|   0|             0|     0|  0|                0|        0|              0|         0|           0|       0|                 0|         0|               0|                0|            0|             0|    2|        2|        0.0|            0.0|        1.0|            1.0|          1.0|          0.0|               0.0|           255|                 2|                  0.01|                  0.72|                        1.0|                        0.0|                 0.0|                     0.0|                 1.0|                     1.0|   portsweep.|\n",
      "|   13500|          tcp|  telnet|RSTO|      125|    10835|   0|             0|     0|  0|                0|        1|              0|         0|           0|       0|                 0|         0|               0|                0|            0|             0|    1|        1|        0.0|            0.0|        1.0|            1.0|          1.0|          0.0|               0.0|            28|                11|                  0.39|                  0.14|                       0.04|                        0.0|                0.04|                    0.09|                0.04|                    0.09|      normal.|\n",
      "|     718|          tcp|  telnet|  SF|     1412|    25260|   0|             0|     0| 15|                0|        1|             38|         1|           0|      54|                 4|         1|               2|                0|            0|             0|    1|        1|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|             1|                 1|                   1.0|                   0.0|                        1.0|                        0.0|                 0.0|                     0.0|                 0.0|                     0.0|    multihop.|\n",
      "|    9170|          tcp|     IRC|RSTO|     1508|     5434|   0|             0|     0|  0|                0|        1|              0|         0|           0|       0|                 0|         0|               0|                0|            0|             0|    1|        1|        0.0|            0.0|        1.0|            1.0|          1.0|          0.0|               0.0|            91|                 2|                  0.02|                  0.02|                       0.01|                        0.0|                 0.0|                     0.0|                0.02|                     1.0|      normal.|\n",
      "|       0|          tcp|     ftp|  S1|     1223|     3371|   0|             0|     0|  1|                0|        1|              0|         0|           0|       0|                 0|         0|               0|                0|            0|             1|    1|        1|        1.0|            1.0|        0.0|            0.0|          1.0|          0.0|               0.0|           166|                45|                  0.27|                  0.04|                       0.01|                        0.0|                0.01|                    0.02|                 0.0|                     0.0|      normal.|\n",
      "|       3|          tcp|  telnet|RSTO|        0|       15|   0|             0|     0|  0|                0|        0|              0|         0|           0|       0|                 0|         0|               0|                0|            0|             0|    1|        1|        0.0|            0.0|        1.0|            1.0|          1.0|          0.0|               0.0|           164|                 1|                  0.01|                  0.04|                       0.01|                        0.0|                 0.0|                     0.0|                0.01|                     1.0|      normal.|\n",
      "|   27264|          tcp|     IRC|  SF|     2965|     8919|   0|             0|     0|  0|                0|        1|              0|         0|           0|       0|                 0|         0|               0|                0|            0|             0|    1|        1|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|           210|                23|                  0.11|                  0.01|                        0.0|                        0.0|                 0.0|                     0.0|                0.07|                    0.61|      normal.|\n",
      "+--------+-------------+--------+----+---------+---------+----+--------------+------+---+-----------------+---------+---------------+----------+------------+--------+------------------+----------+----------------+-----------------+-------------+--------------+-----+---------+-----------+---------------+-----------+---------------+-------------+-------------+------------------+--------------+------------------+----------------------+----------------------+---------------------------+---------------------------+--------------------+------------------------+--------------------+------------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clustered_with_id.join(anomalies_df, 'id').select(origCols).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['duration',\n",
       " 'protocol_type',\n",
       " 'service',\n",
       " 'flag',\n",
       " 'src_bytes',\n",
       " 'dst_bytes',\n",
       " 'land',\n",
       " 'wrong_fragment',\n",
       " 'urgent',\n",
       " 'hot',\n",
       " 'num_failed_logins',\n",
       " 'logged_in',\n",
       " 'num_compromised',\n",
       " 'root_shell',\n",
       " 'su_attempted',\n",
       " 'num_root',\n",
       " 'num_file_creations',\n",
       " 'num_shells',\n",
       " 'num_access_files',\n",
       " 'num_outbound_cmds',\n",
       " 'is_host_login',\n",
       " 'is_guest_login',\n",
       " 'count',\n",
       " 'srv_count',\n",
       " 'serror_rate',\n",
       " 'srv_serror_rate',\n",
       " 'rerror_rate',\n",
       " 'srv_rerror_rate',\n",
       " 'same_srv_rate',\n",
       " 'diff_srv_rate',\n",
       " 'srv_diff_host_rate',\n",
       " 'dst_host_count',\n",
       " 'dst_host_srv_count',\n",
       " 'dst_host_same_srv_rate',\n",
       " 'dst_host_diff_srv_rate',\n",
       " 'dst_host_same_src_port_rate',\n",
       " 'dst_host_srv_diff_host_rate',\n",
       " 'dst_host_serror_rate',\n",
       " 'dst_host_srv_serror_rate',\n",
       " 'dst_host_rerror_rate',\n",
       " 'dst_host_srv_rerror_rate',\n",
       " 'label',\n",
       " 'cluster',\n",
       " 'distance']"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncols = data.columns\n",
    "ncols.append('cluster')\n",
    "ncols.append('distance')\n",
    "ncols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = clustered_with_id.join(anomalies_df, 'id').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+--------+--------+-------------+--------+------+-----------------+---------------+\n",
      "|         id|        label|cluster|distance|duration|protocol_type| service|  flag|num_failed_logins|num_compromised|\n",
      "+-----------+-------------+-------+--------+--------+-------------+--------+------+-----------------+---------------+\n",
      "|      43084|        nmap.|     38| 4606.66|       0|          tcp|     ctf|    SH|                0|              0|\n",
      "| 8589945311|      normal.|     35| 4504.43|      19|          tcp|  telnet|    SF|                2|              0|\n",
      "|      41721|      normal.|     35| 4494.69|      26|          tcp|  telnet|    SF|                2|              0|\n",
      "| 8589945168|        imap.|     25|  4453.6|       0|          tcp|   imap4|    S1|                0|              0|\n",
      "|17179882374|   portsweep.|     49| 3820.58|   40448|          tcp|csnet_ns|  RSTR|                0|              0|\n",
      "|17179882372|   portsweep.|    105| 3515.99|   40682|          tcp|  supdup|  RSTR|                0|              0|\n",
      "|      22772|guess_passwd.|     74|  3357.7|      60|          tcp|  telnet|    S3|                1|              0|\n",
      "|      22750|guess_passwd.|     74| 3337.09|      60|          tcp|  telnet|    S3|                1|              0|\n",
      "|17179885827|    multihop.|    112| 2778.12|     718|          tcp|  telnet|    SF|                0|             38|\n",
      "|42949690996|      normal.|     74| 2776.48|       0|          tcp|    time|    S3|                0|              0|\n",
      "|60129545654|      normal.|     15| 2674.87|    3916|          tcp|  telnet|    SF|                1|              0|\n",
      "|17179882375|   portsweep.|    132| 2564.58|   40339|          tcp|   pop_3|  RSTR|                0|              0|\n",
      "|17179882440|   portsweep.|     47| 2546.83|   30935|          tcp| printer|  RSTR|                0|              0|\n",
      "|17179882445|   portsweep.|     32| 2501.03|   30418|          tcp|    uucp|  RSTR|                0|              0|\n",
      "|17179882443|   portsweep.|     93| 2494.49|   30619|          tcp| courier|  RSTR|                0|              0|\n",
      "|      43132|        nmap.|     29| 2404.33|       0|          tcp|   pop_3|    SH|                0|              0|\n",
      "|51539613149|   portsweep.|     71| 2245.67|   36438|          tcp| private|RSTOS0|                0|              0|\n",
      "|      23235|      normal.|     73| 2195.19|       0|          tcp|   pop_3|    SF|                0|              1|\n",
      "|      11882|      normal.|     35| 2171.97|       1|          tcp|     ftp|    SF|                1|              0|\n",
      "|      21931|      normal.|     73|  2092.4|       0|          tcp|   pop_3|    SF|                0|              1|\n",
      "+-----------+-------------+-------+--------+--------+-------------+--------+------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "anomalies.selectExpr('id', 'label', 'cluster', 'round(distance, 2) as distance', 'duration', 'protocol_type', 'service', 'flag', 'num_failed_logins','num_compromised').orderBy(desc('distance')).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

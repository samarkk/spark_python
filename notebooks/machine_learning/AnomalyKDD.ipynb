{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('ForestCoverTypeClassifier') \\\n",
    ".config('spark.warehouse.dir','/apps/hive/warehouse') \\\n",
    ".config('spark.driver.memory', '4G') \\\n",
    ".config('spark.sql.shuffle.partitions', 4) \\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.1.3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Having set the driver and driver options \n",
    "# we should have spark representing spark session \n",
    "# available straight away\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   KDD cup was like kaggle before there was kaggle\n",
    "#   1999 topic was network intrusion and data is still available\n",
    "#   data available at http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n",
    "#   708 mb in size with 4.89 million  csv rows - each spanning 42 values\n",
    "#   there is also the ten percent sample data set with 490k rows\n",
    "#   data column names are available from kdd.names file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileloc = \"C:/Users/Administrator/Downloads/kddcup.data_10_percent\"\n",
    "data = spark.read \\\n",
    ".option(\"inferSchema\", \"true\") \\\n",
    ".option(\"header\", \"false\") \\\n",
    ".csv(fileloc) \\\n",
    ".toDF(\n",
    "      \"duration\", \"protocol_type\", \"service\", \"flag\",\n",
    "      \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\",\n",
    "      \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\",\n",
    "      \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\",\n",
    "      \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
    "      \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\",\n",
    "      \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\",\n",
    "      \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\",\n",
    "      \"dst_host_count\", \"dst_host_srv_count\",\n",
    "      \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\",\n",
    "      \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\",\n",
    "      \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n",
    "      \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\",\n",
    "      \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The labels and their count - 23 distinct labels with most frequent being smurf. and neptune.\n",
      "+----------------+------+\n",
      "|           label| count|\n",
      "+----------------+------+\n",
      "|          smurf.|280790|\n",
      "|        neptune.|107201|\n",
      "|         normal.| 97278|\n",
      "|           back.|  2203|\n",
      "|          satan.|  1589|\n",
      "|        ipsweep.|  1247|\n",
      "|      portsweep.|  1040|\n",
      "|    warezclient.|  1020|\n",
      "|       teardrop.|   979|\n",
      "|            pod.|   264|\n",
      "|           nmap.|   231|\n",
      "|   guess_passwd.|    53|\n",
      "|buffer_overflow.|    30|\n",
      "|           land.|    21|\n",
      "|    warezmaster.|    20|\n",
      "|           imap.|    12|\n",
      "|        rootkit.|    10|\n",
      "|     loadmodule.|     9|\n",
      "|      ftp_write.|     8|\n",
      "|       multihop.|     7|\n",
      "|            phf.|     4|\n",
      "|           perl.|     3|\n",
      "|            spy.|     2|\n",
      "+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  take a look at the labels\n",
    "#  in unsupervised learning we do not use the label - however it is here useful to\n",
    "#  find the labels present in the data - \n",
    "#  normal and various type of classified network attacks\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "print(\"\\nThe labels and their count - 23 distinct labels with most \\\n",
    "frequent being smurf. and neptune.\")\n",
    "data.select(\"label\").groupBy(\"label\").count().orderBy(desc('count')).show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the non numeric columns\n",
    "numericOnly = data.drop(\"protocol_type\", \"service\", \"flag\").cache()\n",
    "# assemble them into a vector leaving the label out\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "inputCols = numericOnly.columns\n",
    "inputCols.remove('label')\n",
    "assembler = VectorAssembler() \\\n",
    ".setInputCols(inputCols) \\\n",
    ".setOutputCol(\"featureVector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint, seed\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans() \\\n",
    ".setSeed(100) \\\n",
    ".setPredictionCol(\"cluster\") \\\n",
    ".setFeaturesCol(\"featureVector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "pipeline = Pipeline().setStages([assembler, kmeans])\n",
    "pipelineModel = pipeline.fit(numericOnly)\n",
    "kmeansModel = pipelineModel.stages[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with no parameter setting, kmeans creates two clusters\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([4.79793956e+01, 1.62207883e+03, 8.68534183e+02, 4.45326100e-05,\n",
       "        6.43293794e-03, 1.41694668e-05, 3.45168212e-02, 1.51815716e-04,\n",
       "        1.48247035e-01, 1.02121372e-02, 1.11331525e-04, 3.64357718e-05,\n",
       "        1.13517671e-02, 1.08295211e-03, 1.09307315e-04, 1.00805635e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.38658354e-03, 3.32286248e+02,\n",
       "        2.92907143e+02, 1.76685418e-01, 1.76607809e-01, 5.74330999e-02,\n",
       "        5.77183920e-02, 7.91548844e-01, 2.09816404e-02, 2.89968625e-02,\n",
       "        2.32470732e+02, 1.88666046e+02, 7.53781203e-01, 3.09056111e-02,\n",
       "        6.01935529e-01, 6.68351484e-03, 1.76753957e-01, 1.76441622e-01,\n",
       "        5.81176268e-02, 5.74111170e-02]),\n",
       " array([2.0000000e+00, 6.9337564e+08, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 1.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 5.7000000e+01,\n",
       "        3.0000000e+00, 7.9000000e-01, 6.7000000e-01, 2.1000000e-01,\n",
       "        3.3000000e-01, 5.0000000e-02, 3.9000000e-01, 0.0000000e+00,\n",
       "        2.5500000e+02, 3.0000000e+00, 1.0000000e-02, 9.0000000e-02,\n",
       "        2.2000000e-01, 0.0000000e+00, 1.8000000e-01, 6.7000000e-01,\n",
       "        5.0000000e-02, 3.3000000e-01])]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"with no parameter setting, kmeans creates two clusters\\n\")\n",
    "kmeansModel.clusterCenters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Taking a look at the clusters generated and the labels assigned\n",
      "+-------+----------------+------+\n",
      "|cluster|label           |count |\n",
      "+-------+----------------+------+\n",
      "|0      |smurf.          |280790|\n",
      "|0      |neptune.        |107201|\n",
      "|0      |normal.         |97278 |\n",
      "|0      |back.           |2203  |\n",
      "|0      |satan.          |1589  |\n",
      "|0      |ipsweep.        |1247  |\n",
      "|0      |portsweep.      |1039  |\n",
      "|0      |warezclient.    |1020  |\n",
      "|0      |teardrop.       |979   |\n",
      "|0      |pod.            |264   |\n",
      "|0      |nmap.           |231   |\n",
      "|0      |guess_passwd.   |53    |\n",
      "|0      |buffer_overflow.|30    |\n",
      "|0      |land.           |21    |\n",
      "|0      |warezmaster.    |20    |\n",
      "|0      |imap.           |12    |\n",
      "|0      |rootkit.        |10    |\n",
      "|0      |loadmodule.     |9     |\n",
      "|0      |ftp_write.      |8     |\n",
      "|0      |multihop.       |7     |\n",
      "|0      |phf.            |4     |\n",
      "|0      |perl.           |3     |\n",
      "|0      |spy.            |2     |\n",
      "|1      |portsweep.      |1     |\n",
      "+-------+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use the model transformer to add cluster information to the dataset we gave to the\n",
    "# estimator to generate the model\n",
    "\n",
    "withCluster = pipelineModel.transform(numericOnly)\n",
    "\n",
    "print(\"\\nTaking a look at the clusters generated and the labels assigned\")\n",
    "withCluster.select(\"cluster\", \"label\"). \\\n",
    "groupBy(\"cluster\", \"label\").count(). \\\n",
    "orderBy(\"cluster\", desc('count')). \\\n",
    "show(50, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999979569933308\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "evaluator = ClusteringEvaluator(featuresCol='featureVector',\n",
    "                                predictionCol='cluster')\n",
    "silhouette = evaluator.evaluate(pipelineModel.transform(numericOnly))\n",
    "print(silhouette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us try with different values of k the best possible score that we wil get\n",
    "def clusteringScore0(data, k):\n",
    "    input_cols = data.columns\n",
    "    input_cols.remove('label')\n",
    "    assembler = VectorAssembler().\\\n",
    "    setInputCols(input_cols). \\\n",
    "    setOutputCol(\"featureVector\")\n",
    "\n",
    "    kmeans = KMeans(). \\\n",
    "    setSeed(100). \\\n",
    "    setK(k). \\\n",
    "    setPredictionCol(\"cluster\"). \\\n",
    "    setFeaturesCol(\"featureVector\")\n",
    "\n",
    "    pipeline = Pipeline().setStages([assembler, kmeans])\n",
    "\n",
    "    kmeansModel = pipeline.fit(data).stages[1]\n",
    "    evaluator = ClusteringEvaluator(featuresCol='featureVector',\n",
    "                                    predictionCol='cluster')\n",
    "    silhouette = evaluator.evaluate(pipelineModel.transform(numericOnly))\n",
    "    return silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Take 0 - the cost with varied number of clusters\n",
      "[(20, 0.9999979569933308), (40, 0.9999979569933308), (60, 0.9999979569933308), (80, 0.9999979569933308), (100, 0.9999979569933308), (120, 0.9999979569933308), (140, 0.9999979569933308)]\n"
     ]
    }
   ],
   "source": [
    "# take 0\n",
    "print(\"\\nTake 0 - the cost with varied number of clusters\")\n",
    "  \n",
    "# We see that the cost is lowest for 80\n",
    "# if we were to take as many clusters as the number of points,\n",
    "# the cost would be 0\n",
    "\n",
    "print(list(map(lambda k: (k, clusteringScore0(numericOnly, k)),\n",
    "               range(20, 160, 20))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusteringScore1(data, k):\n",
    "    input_cols = data.columns\n",
    "    input_cols.remove('label')\n",
    "    \n",
    "    assembler = VectorAssembler().\\\n",
    "    setInputCols(input_cols). \\\n",
    "    setOutputCol(\"featureVector\")\n",
    "\n",
    "    kmeans = KMeans(). \\\n",
    "    setSeed(100). \\\n",
    "    setK(k). \\\n",
    "    setPredictionCol(\"cluster\"). \\\n",
    "    setFeaturesCol(\"featureVector\"). \\\n",
    "    setMaxIter(40).setTol(1.0e-5)\n",
    "\n",
    "    pipeline = Pipeline().setStages([assembler, kmeans])\n",
    "\n",
    "    kmeansModel = pipeline.fit(data).stages[1]\n",
    "    evaluator = ClusteringEvaluator(featuresCol='featureVector',\n",
    "                                    predictionCol='cluster')\n",
    "    silhouette = evaluator.evaluate(pipelineModel.transform(numericOnly))\n",
    "    return silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Take 1 - the cost where we have explicitly set      the number of iterations and tolerance\n",
      "[(20, 0.9999979569933308), (40, 0.9999979569933308), (60, 0.9999979569933308), (80, 0.9999979569933308), (100, 0.9999979569933308), (120, 0.9999979569933308), (140, 0.9999979569933308)]\n"
     ]
    }
   ],
   "source": [
    "# take 1\n",
    "print(\"\\nTake 1 - the cost where we have explicitly set\\\n",
    "      the number of iterations and tolerance\")\n",
    "\n",
    "# when we run more iterations and set the tolerance level, we see\n",
    "# that the clustering cost is lowest for 60\n",
    "# higher number of clusters should have a lower cost\n",
    "# the problem may be suboptimal clustering due to ineffective starting points or not low\n",
    "# enough distance discriminator or both\n",
    "print(list(map(lambda k: (k, clusteringScore1(numericOnly, k)),\n",
    "               range(20, 160, 20))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we are going to normalize the data and check\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "def clusteringScore2(data, k):\n",
    "    input_cols = data.columns\n",
    "    input_cols.remove('label')\n",
    "    \n",
    "    assembler = VectorAssembler().\\\n",
    "    setInputCols(input_cols). \\\n",
    "    setOutputCol(\"featureVector\")\n",
    "    \n",
    "    scaler = StandardScaler(). \\\n",
    "    setInputCol(\"featureVector\"). \\\n",
    "    setOutputCol(\"scaledFeatureVector\"). \\\n",
    "    setWithStd(True). \\\n",
    "    setWithMean(False)\n",
    "    \n",
    "    kmeans = KMeans(). \\\n",
    "    setSeed(100). \\\n",
    "    setK(k). \\\n",
    "    setPredictionCol(\"cluster\"). \\\n",
    "    setFeaturesCol(\"scaledFeatureVector\"). \\\n",
    "    setMaxIter(40).setTol(1.0e-5)\n",
    "\n",
    "    pipeline = Pipeline().setStages([assembler, scaler,  kmeans])\n",
    "    pipelineModel = pipeline.fit(data)\n",
    "    \n",
    "    kmeansModel = pipeline.fit(data).stages[2]\n",
    "    \n",
    "    evaluator = ClusteringEvaluator(featuresCol='scaledFeatureVector',\n",
    "                                    predictionCol='cluster')\n",
    "    silhouette = evaluator.evaluate(pipelineModel.transform(numericOnly))\n",
    "    return silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Take 2 - Consider the scores after we have scaled, normalized the data\n",
      "[(20, 0.8323676746559989), (40, 0.8759520863371246), (60, 0.8877020359031141), (80, 0.899296604367312), (100, 0.8893267328149023), (120, 0.8532272306638351), (140, 0.7777412274813398)]\n"
     ]
    }
   ],
   "source": [
    "# take 2\n",
    "print(\"\\nTake 2 - Consider the scores after we have scaled, normalized the data\")\n",
    "print(list(map(lambda k: (k, clusteringScore2(numericOnly, k)),\n",
    "               range(20, 160, 20))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will include the non numeric columns and check\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "def oneHotPipeline(inputCol): \n",
    "    indexer =  StringIndexer(). \\\n",
    "    setInputCol(inputCol).\\\n",
    "    setOutputCol(inputCol + \"_indexed\")\n",
    "\n",
    "    encoder =  OneHotEncoder().\\\n",
    "    setInputCol(inputCol + \"_indexed\").\\\n",
    "    setOutputCol(inputCol + \"_vec\")\n",
    "\n",
    "    pipeline = Pipeline().setStages([indexer, encoder])\n",
    "    return(pipeline, inputCol + \"_vec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusteringScore3(data, k):\n",
    "    protoTypeEncoder, protoTypeVecCol = oneHotPipeline(\"protocol_type\")\n",
    "    serviceEncoder, serviceVecCol = oneHotPipeline(\"service\")\n",
    "    flagEncoder, flagVecCol = oneHotPipeline(\"flag\")\n",
    "    \n",
    "    assembleCols = data.columns\n",
    "#     print(assembleCols)\n",
    "    [assembleCols.remove(col) for col in ['label', 'protocol_type', 'service', 'flag' ]]\n",
    "    [assembleCols.append(col) for col in [protoTypeVecCol, serviceVecCol, flagVecCol]]\n",
    "#     print(assembleCols)\n",
    "    \n",
    "    assembler = VectorAssembler().\\\n",
    "    setInputCols(assembleCols). \\\n",
    "    setOutputCol(\"featureVector\")\n",
    "    \n",
    "    scaler = StandardScaler(). \\\n",
    "    setInputCol(\"featureVector\"). \\\n",
    "    setOutputCol(\"scaledFeatureVector\"). \\\n",
    "    setWithStd(True). \\\n",
    "    setWithMean(False)\n",
    "    \n",
    "    kmeans = KMeans(). \\\n",
    "    setSeed(100). \\\n",
    "    setK(k). \\\n",
    "    setPredictionCol(\"cluster\"). \\\n",
    "    setFeaturesCol(\"scaledFeatureVector\"). \\\n",
    "    setMaxIter(40).setTol(1.0e-5)\n",
    "\n",
    "    pipeline = Pipeline().setStages([protoTypeEncoder,\n",
    "            serviceEncoder, flagEncoder, assembler, scaler,  kmeans])\n",
    "    pipelineModel = pipeline.fit(data)\n",
    "    \n",
    "    kmeansModel = pipeline.fit(data).stages[-1]\n",
    "    \n",
    "    evaluator = ClusteringEvaluator(featuresCol='scaledFeatureVector',\n",
    "                                    predictionCol='cluster')\n",
    "    silhouette = evaluator.evaluate(pipelineModel.transform(data))\n",
    "    return silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Take 3 - Clustering cost scores after one hot encoding string columns\n",
      "[(20, 0.4034589770286568)]\n"
     ]
    }
   ],
   "source": [
    "# take 3\n",
    "print(\"\\nTake 3 - Clustering cost scores after one hot encoding string columns\")\n",
    "#  we have all columns included and now the elbow seems to be around 180/210\n",
    "print(list(map(lambda k: (k, clusteringScore3(data, k)),\n",
    "               range(20, 160, 200))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use entropy to see the quality of individual clusters\n",
    "# we will use the labels to compute entropy\n",
    "# more the dominance of a single label, lower will be the entropy\n",
    "# more distributed the presence of different labels, higher will be the entropy \n",
    "# thus more a cluster exhibits skewness towards a single entity, \n",
    "# lower will be the entropy and vice versa\n",
    "import math\n",
    "import builtins\n",
    "def entropy(counts):\n",
    "    from builtins import sum as pysum\n",
    "    from math import log as pylog\n",
    "    from builtins import filter as pyfilter\n",
    "    from builtins import map as pymap\n",
    "    values = list(pyfilter(lambda x: x > 0, counts))\n",
    "    n = pysum(values)\n",
    "    return pysum(pymap(lambda v: - v / n * pylog(v / n), values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.277034259466139\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.6578814551411559"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list_to_check =[100, 200, 300, 100]\n",
    "# values = list(filter(lambda x: x > 0, list_to_check))\n",
    "# values\n",
    "print(entropy([100, 200, 300, 100]))\n",
    "-100/700 * math.log(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitPipeline4(data, k):\n",
    "    protoTypeEncoder, protoTypeVecCol = oneHotPipeline(\"protocol_type\")\n",
    "    serviceEncoder, serviceVecCol = oneHotPipeline(\"service\")\n",
    "    flagEncoder, flagVecCol = oneHotPipeline(\"flag\")\n",
    "    \n",
    "    assembleCols = data.columns\n",
    "#     print(assembleCols)\n",
    "    [assembleCols.remove(col) for col in ['label', 'protocol_type', 'service', 'flag' ]]\n",
    "    [assembleCols.append(col) for col in \n",
    "     [protoTypeVecCol, serviceVecCol, flagVecCol]]\n",
    "#     print(assembleCols)\n",
    "    \n",
    "    assembler = VectorAssembler().\\\n",
    "    setInputCols(assembleCols). \\\n",
    "    setOutputCol(\"featureVector\")\n",
    "    \n",
    "    scaler = StandardScaler(). \\\n",
    "    setInputCol(\"featureVector\"). \\\n",
    "    setOutputCol(\"scaledFeatureVector\"). \\\n",
    "    setWithStd(True). \\\n",
    "    setWithMean(False)\n",
    "    \n",
    "    kmeans = KMeans(). \\\n",
    "    setSeed(100). \\\n",
    "    setK(k). \\\n",
    "    setPredictionCol(\"cluster\"). \\\n",
    "    setFeaturesCol(\"scaledFeatureVector\"). \\\n",
    "    setMaxIter(40).setTol(1.0e-5)\n",
    "\n",
    "    pipeline = Pipeline().setStages([protoTypeEncoder, serviceEncoder, flagEncoder, assembler, scaler,  kmeans])\n",
    "    pipelineModel = pipeline.fit(data)\n",
    "    \n",
    "    return pipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = fitPipeline4(data, 20).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|cluster|label  |\n",
      "+-------+-------+\n",
      "|2      |normal.|\n",
      "|2      |normal.|\n",
      "|2      |normal.|\n",
      "|2      |normal.|\n",
      "|2      |normal.|\n",
      "|2      |normal.|\n",
      "|2      |normal.|\n",
      "|2      |normal.|\n",
      "|2      |normal.|\n",
      "|2      |normal.|\n",
      "|2      |normal.|\n",
      "|2      |normal.|\n",
      "|2      |normal.|\n",
      "|2      |normal.|\n",
      "|2      |normal.|\n",
      "|2      |normal.|\n",
      "|2      |normal.|\n",
      "|2      |normal.|\n",
      "|2      |normal.|\n",
      "|2      |normal.|\n",
      "+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.select('cluster', 'label').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494021"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100259.8301838427"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.select('cluster', 'label').rdd.map(lambda x: (x[0], x[1])) \\\n",
    ".groupByKey().map(lambda x: (x[0], [list(x[1]).count(v) \n",
    "            for v in set(x[1])], len(list(x[1])))) \\\n",
    ".map(lambda x: (x[0], entropy(x[1]), x[2])) \\\n",
    ".map(lambda x: (x[0], (x[1] * x[2]))) \\\n",
    ".map(lambda x: x[1]) \\\n",
    ".sum()\n",
    "# .map(lambda x: (x[0], len(x[1][1]))).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusteringScore4(data, k):\n",
    "    cluster_df = fitPipeline4(data, k).transform(data)\n",
    "    return cluster_df.select('cluster', 'label').rdd.map(lambda x: (x[0], x[1])) \\\n",
    "    .groupByKey().map(lambda x: (x[0], [list(x[1]).count(v) for v in set(x[1])], len(list(x[1])))) \\\n",
    "    .map(lambda x: (x[0], entropy(x[1]), x[2])) \\\n",
    "    .map(lambda x: x[1] * x[2]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100259.8301838427"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusteringScore4(data, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Take 4 - Now going for clustering score 4 - estimating the entropy - best score to be found for k = 100\n",
      "14846.811180693545\n"
     ]
    }
   ],
   "source": [
    "# take 4\n",
    "# the best clustering is to be found around 180 - and every iteration takes time\n",
    "# so on a local cluster use the known\n",
    "print(\"\\nTake 4 - Now going for clustering score 4 - estimating the entropy - best score to be found for k = 100\")\n",
    "print(clusteringScore4(data, 180))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|cluster|scaledFeatureVector                                                                                                                                                                                                                                                                                       |\n",
      "+-------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|103    |(115,[1,2,8,19,20,25,28,29,30,32,39,42,105],[1.8315794844034117E-4,0.16495156759878016,2.814168444874875,0.03753270996838475,0.03247770581832668,2.576061480099788,0.13900605646702138,0.0848732827397667,2.434387313317322,0.22854329046843286,2.055363006399789,2.9721197203276812,2.362126925892916])  |\n",
      "|103    |(115,[1,2,8,19,20,25,28,29,30,32,39,42,105],[2.4184944573061624E-4,0.014709442541836176,2.814168444874875,0.03753270996838475,0.03247770581832668,2.576061480099788,0.29345723031926735,0.17917693022839637,2.434387313317322,0.10388331384928767,2.055363006399789,2.9721197203276812,2.362126925892916])|\n",
      "|103    |(115,[1,2,8,19,20,25,28,29,30,32,39,42,105],[2.378017562623214E-4,0.040466100161388824,2.814168444874875,0.03753270996838475,0.03247770581832668,2.576061480099788,0.44790840417151334,0.27348057771702605,2.434387313317322,0.06232998830957259,2.055363006399789,2.9721197203276812,2.362126925892916]) |\n",
      "|103    |(115,[1,2,8,19,20,25,28,29,30,32,39,42,105],[2.2161099838914207E-4,0.040466100161388824,2.814168444874875,0.028149532476288562,0.02435827936374501,2.576061480099788,0.6023595780237593,0.3677842252056557,2.434387313317322,0.06232998830957259,2.055363006399789,2.9721197203276812,2.362126925892916]) |\n",
      "|103    |(115,[1,2,8,19,20,25,28,29,30,32,39,42,105],[2.1958715365499466E-4,0.06150120832306813,2.814168444874875,0.028149532476288562,0.02435827936374501,2.576061480099788,0.7568107518760053,0.46208787269428536,2.434387313317322,0.041553325539715066,2.055363006399789,2.9721197203276812,2.362126925892916])|\n",
      "+-------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "threshold is  622.815462555213\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vector, Vectors\n",
    "pipelineModel = fitPipeline4(data, 180)\n",
    "\n",
    "kmeansModel = pipelineModel.stages[-1]\n",
    "centroids = kmeansModel.clusterCenters()\n",
    "# print(centroids[0])\n",
    "# print(len(centroids[0]))\n",
    "\n",
    "clustered = pipelineModel.transform(data)\n",
    "clustered.select('cluster', 'scaledFeatureVector').show(5, False)\n",
    "\n",
    "threshold_boundary = 100\n",
    "\n",
    "threshold = clustered. \\\n",
    "select(\"cluster\", \"scaledFeatureVector\").rdd. \\\n",
    "map(lambda x: Vectors.squared_distance(Vectors.dense(centroids[x[0]]), x[1])) \\\n",
    ".sortBy(lambda x: -x).take(threshold_boundary)[threshold_boundary - 1]\n",
    "\n",
    "print('threshold is ' , threshold)\n",
    "\n",
    "\n",
    "# map { case (cluster, vec) => Vectors.sqdist(centroids(cluster), vec) }.\n",
    "#       orderBy($\"value\".desc).take(100).last\n",
    "\n",
    "#     val originalCols = data.columns\n",
    "#     val anomalies = clustered.filter { row =>\n",
    "#       val cluster = row.getAs[Int](\"cluster\")\n",
    "#       val vec = row.getAs[Vector](\"scaledFeatureVector\")\n",
    "#       Vectors.sqdist(centroids(cluster), vec) >= threshold\n",
    "#     }.select(originalCols.head, originalCols.tail: _*)\n",
    "\n",
    "#     println(\"\\nPrinting the first ten anomalous looking entries\")\n",
    "#     anomalies.take(10).foreach(println)\n",
    "\n",
    "#     println(\"Total number of anomalous points found: \" + anomalies.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clustered.withColumn('id', monotonically_increasing_id()).select('id').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_with_id = clustered.withColumn('id', monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11882, 1392.3129530340516),\n",
       " (16064, 857.4055273940494),\n",
       " (21931, 2092.4015736714387),\n",
       " (22801, 779.3232536400462),\n",
       " (22826, 628.7328953505383)]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomalies_rdd = clustered_with_id.select(\"id\", \"cluster\", \"scaledFeatureVector\").rdd. \\\n",
    "map(lambda x: (x[0], Vectors.squared_distance(Vectors.dense(centroids[x[1]]), x[2]))) \\\n",
    ".filter(lambda x: x[1] >= threshold)\n",
    "anomalies_rdd.cache()\n",
    "anomalies_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total numboer of anomailes  100\n"
     ]
    }
   ],
   "source": [
    "print('total numboer of anomailes ', anomalies_rdd.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unable to infer the type of the field distance.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\types.py\u001b[0m in \u001b[0;36m_infer_type\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   1040\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1041\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_infer_schema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1042\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\types.py\u001b[0m in \u001b[0;36m_infer_schema\u001b[1;34m(row, names)\u001b[0m\n\u001b[0;32m   1066\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1067\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Can not infer schema for type: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Can not infer schema for type: <class 'numpy.float64'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\types.py\u001b[0m in \u001b[0;36m_infer_schema\u001b[1;34m(row, names)\u001b[0m\n\u001b[0;32m   1071\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1072\u001b[1;33m             \u001b[0mfields\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mStructField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_infer_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1073\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\types.py\u001b[0m in \u001b[0;36m_infer_type\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   1042\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1043\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"not supported type: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: not supported type: <class 'numpy.float64'>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_5388/2159321231.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0manomalies_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manomalies_rdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'distance'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0manomalies_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mtoDF\u001b[1;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Alice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \"\"\"\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    673\u001b[0m             return super(SparkSession, self).createDataFrame(\n\u001b[0;32m    674\u001b[0m                 data, schema, samplingRatio, verifySchema)\n\u001b[1;32m--> 675\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_create_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    697\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 698\u001b[1;33m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    699\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[1;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[0;32m    484\u001b[0m         \"\"\"\n\u001b[0;32m    485\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 486\u001b[1;33m             \u001b[0mstruct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    487\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[1;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msamplingRatio\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m             \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_infer_schema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    467\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\types.py\u001b[0m in \u001b[0;36m_infer_schema\u001b[1;34m(row, names)\u001b[0m\n\u001b[0;32m   1072\u001b[0m             \u001b[0mfields\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mStructField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_infer_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1073\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1074\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unable to infer the type of the field {}.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1075\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mStructType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Unable to infer the type of the field distance."
     ]
    }
   ],
   "source": [
    "anomalies_df = anomalies_rdd.toDF(['id', 'distance'])\n",
    "anomalies_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_df = anomalies_rdd.map(lambda x: (x[0], float(x[1]))).toDF(['id', 'distance'])\n",
    "anomalies_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origCols = data.columns\n",
    "data.select(origCols).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_with_id.join(anomalies_df, 'id').select(origCols).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = data.columns\n",
    "ncols.append('cluster')\n",
    "ncols.append('distance')\n",
    "ncols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = clustered_with_id.join(anomalies_df, 'id').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies.selectExpr('id', 'label', 'cluster', 'round(distance, 2) as distance', 'duration', 'protocol_type', 'service', 'flag', 'num_failed_logins','num_compromised').orderBy(desc('distance')).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
